{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Pending approach- learn phrases and then learn vectors, pass them in classifier and show frequent phrases from each pos and neg corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name TweetTokenizer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-394a980d2b24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTweetTokenizer\u001b[0m \u001b[0;31m# a tweet tokenizer from nltk.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTweetTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name TweetTokenizer"
     ]
    }
   ],
   "source": [
    "import pandas as pd # provide sql-like data manipulation tools. very handy.\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np # high dimensional vector computing library.\n",
    "from copy import deepcopy\n",
    "from string import punctuation\n",
    "from random import shuffle\n",
    "import json\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec # the word2vec model gensim class\n",
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence # we'll talk about this down below\n",
    "# from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer # a tweet tokenizer from nltk.\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import scale\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "os.environ['MKL_NUM_THREADS'] = '12'\n",
    "os.environ['GOTO_NUM_THREADS'] = '12'\n",
    "os.environ['OMP_NUM_THREADS'] = '12'\n",
    "os.environ['openmp'] = 'True'\n",
    "\n",
    "from keras.models import model_from_json\n",
    "pd.set_option('display.height', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk import RegexpTokenizer\n",
    "import nltk\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "import collections\n",
    "import ast\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from pattern.en import tokenize,parse\n",
    "\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('dataset loaded with shape', (1600000, 2))\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"deeplearning/data\")\n",
    "directory = os.getcwd()\n",
    "trainPath = r\"/tweets_data/train.csv\"\n",
    "def ingest(path):\n",
    "    data = pd.read_csv(directory + path)\n",
    "    data.drop(['id', 'device',\"datetime\",\"user\"], axis=1, inplace=True)\n",
    "    data = data[data.sentiment.isnull() == False]\n",
    "    data['sentiment'] = data['sentiment'].map(int)\n",
    "    data['sentiment'] = data['sentiment'].map( {4:1, 0:0} )\n",
    "    data = data[data['text'].isnull() == False]\n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop('index', axis=1, inplace=True)\n",
    "    print ('dataset loaded with shape', data.shape)\n",
    "    return data\n",
    "\n",
    "data = ingest(trainPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(tweet):\n",
    "    try:\n",
    "        tweet =  tweet.lower()\n",
    "        tokens = tokenizer.tokenize(tweet)\n",
    "        tokens = list(filter(lambda t: not t.startswith('@'), tokens))\n",
    "        tokens = list(filter(lambda t: not t.startswith('#'), tokens))\n",
    "        tokens = list(filter(lambda t: not t.startswith('http'), tokens))\n",
    "        return tokens\n",
    "    except:\n",
    "        return 'NC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress-bar: 100%|██████████| 1600000/1600000 [02:20<00:00, 11352.23it/s]\n"
     ]
    }
   ],
   "source": [
    "def postprocess(data, n=data.shape[0]):\n",
    "    data = data.head(n)\n",
    "    data['tokens'] = data['text'].progress_map(tokenize)  ## progress_map is a variant of the map function plus a progress bar. Handy to monitor DataFrame creations.\n",
    "    data = data[data.tokens != 'NC']\n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop('index', inplace=True, axis=1)\n",
    "    return data\n",
    "\n",
    "data = postprocess(data)\n",
    "# test_data = postprocess(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x_train, x_val, y_train, y_val = train_test_split(np.array(data.head(n).tokens),\n",
    "#                                                 np.array(data.head(n).sentiment), test_size=0.2)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(data.tokens), np.array(data.sentiment), test_size=0.1, random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import itemfreq\n",
    "\n",
    "print(\"trainset labels distribution\")\n",
    "print(itemfreq(y_train))\n",
    "print\n",
    "print(\"valset labels distribution\")\n",
    "print (itemfreq(y_val))\n",
    "print\n",
    "print(\"testset labels distribution\")\n",
    "print( itemfreq(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def labelizeTweets(tweets, label_type):\n",
    "    labelized = []\n",
    "    for i,v in tqdm(enumerate(tweets)):\n",
    "        label = '%s_%s'%(label_type,i)\n",
    "        labelized.append(LabeledSentence(v, [label]))\n",
    "    return labelized\n",
    "\n",
    "X_train = labelizeTweets(X_train, 'TRAIN')\n",
    "X_val = labelizeTweets(X_val, 'Val')\n",
    "X_test = labelizeTweets(X_test, 'TEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_dim = 200\n",
    "tweet_w2v = Word2Vec(size=n_dim, min_count=10)\n",
    "tweet_w2v.build_vocab([x.words for x in tqdm(X_train)])\n",
    "tweet_w2v.train([x.words for x in tqdm(X_train)],total_examples=tweet_w2v.corpus_count, epochs=tweet_w2v.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_w2v.save(directory + \"/saved_models/equal_dist_Word2Vec_model_200dims\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"deeplearning/data\")\n",
    "directory = os.getcwd()\n",
    "tweet_w2v = Word2Vec.load(directory + \"/saved_models/equal_dist_Word2Vec_model_200dims\")  # you can continue training with the loaded model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'goood', 0.7326517105102539),\n",
       " (u'great', 0.726836085319519),\n",
       " (u'gooood', 0.6671431064605713),\n",
       " (u'rough', 0.6489897966384888),\n",
       " (u'goooood', 0.6336358189582825),\n",
       " (u'fantastic', 0.6276979446411133),\n",
       " (u'tough', 0.6255412697792053),\n",
       " (u'gd', 0.615630030632019),\n",
       " (u'fab', 0.6139873266220093),\n",
       " (u'nice', 0.6114063262939453)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_w2v.most_similar('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'cafe', 0.6940361261367798),\n",
       " (u'pub', 0.6776139736175537),\n",
       " (u'lounge', 0.6597999334335327),\n",
       " (u'club', 0.6557016372680664),\n",
       " (u'restaurant', 0.6481664180755615),\n",
       " (u'table', 0.6430073976516724),\n",
       " (u'grill', 0.6275577545166016),\n",
       " (u'gate', 0.6055143475532532),\n",
       " (u'casino', 0.6018807888031006),\n",
       " (u'bakery', 0.5978732705116272)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_w2v.most_similar('bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'fb', 0.8944979310035706),\n",
       " (u'myspace', 0.8294822573661804),\n",
       " (u'bebo', 0.7485839128494263),\n",
       " (u'twitter', 0.7347208261489868),\n",
       " (u'linkedin', 0.7318819165229797),\n",
       " (u'flickr', 0.7263391017913818),\n",
       " (u'yahoo', 0.7208259105682373),\n",
       " (u'msn', 0.7146008014678955),\n",
       " (u'lj', 0.7127932906150818),\n",
       " (u'aim', 0.6888376474380493)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_w2v.most_similar('facebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF matrix for weighted average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tf-idf matrix ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-3b3b361e570b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'building tf-idf matrix ...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midf_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'vocab size :'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "print ('building tf-idf matrix ...')\n",
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "matrix = vectorizer.fit_transform([x.words for x in X_train])\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "print ('vocab size :', len(tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# directory = r\"/deeplearning/data\"\n",
    "# with open (directory+'/saved_models/equal_dist_tf-idf', 'wb') as fp:\n",
    "#     pickle.dump(tfidf, fp, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Saved tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import cPickle as pickle\n",
    "directory = r\"deeplearning/data\"\n",
    "with open (directory+'/saved_models/equal_dist_tf-idf', 'rb') as fp:\n",
    "    tfidf = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildWordVector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += tweet_w2v[word].reshape((1, size)) * tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_dim = 200\n",
    "train_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in tqdm(map(lambda x: x.words, X_train))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "val_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in tqdm(map(lambda x: x.words, X_val))])\n",
    "val_vecs_w2v = scale(val_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in tqdm(map(lambda x: x.words, X_test))])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get vectors for out of Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/dist-packages/IPython/core/interactiveshell.py:2741: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(460023, 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r\"/tweets_data/amazon_text_for_sentiment.csv\"\n",
    "new_words = pd.read_csv(directory + path)\n",
    "new_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = r\"/tweets_data/amazon_text_for_sentiment.csv\"\n",
    "def ingest(path):\n",
    "    data = pd.read_csv(directory + path)\n",
    "    data.drop(['_id',\"quoted_status_sentiment\"], axis=1, inplace=True) # extra cols'tweet_text_sentiment',\"tweet_quoted_status_text\"\n",
    "    data = data[data['tweet_text'].isnull() == False]\n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop('index', axis=1, inplace=True)\n",
    "    print ('dataset loaded with shape', data.shape)\n",
    "    return data\n",
    "\n",
    "new_data = ingest(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(tweet):\n",
    "    try:\n",
    "        tweet =  tweet.lower()\n",
    "        tokens = tokenizer.tokenize(tweet)\n",
    "        tokens = list(filter(lambda t: not t.startswith('@'), tokens))\n",
    "        tokens = list(filter(lambda t: not t.startswith('#'), tokens))\n",
    "        tokens = list(filter(lambda t: not t.startswith('http'), tokens))\n",
    "        return tokens\n",
    "    except:\n",
    "        return 'NC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def postprocess(data):\n",
    "    data['tokens'] = data['tweet_text'].progress_map(tokenize)  ## progress_map is a variant of the map function plus a progress bar. Handy to monitor DataFrame creations.\n",
    "    data = data[data.tokens != 'NC']\n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop('index', inplace=True, axis=1)\n",
    "    return data\n",
    "\n",
    "new_data = postprocess(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# new_data.to_csv(directory + \"/tweets_data/cleaned_amazon_reviews.csv\")\n",
    "# new_data = pd.read_csv(directory + \"/tweets_data/cleaned_amazon_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# new_data = new_data.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OOS_data = np.array(new_data.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def labelizeTweets(tweets, label_type):\n",
    "    labelized = []\n",
    "    for i,v in tqdm(enumerate(tweets)):\n",
    "        label = '%s_%s'%(label_type,i)\n",
    "        labelized.append(LabeledSentence(v, [label]))\n",
    "    return labelized\n",
    "\n",
    "OOS_data = labelizeTweets(OOS_data, 'OOS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_dim = 200\n",
    "OOS_vectors = np.concatenate([buildWordVector(z, n_dim) for z in tqdm(map(lambda x: x.words, OOS_data))])\n",
    "OOS_vectors = scale(OOS_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving word vectors- don't run it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#OOS vectors\n",
    "# directory = r\"deeplearning/data\"\n",
    "# with open(directory+'/word2vec/equal_dist_OOS_wordvectors_v3', 'wb') as fp:\n",
    "#     pickle.dump(OOS_vectors, fp, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# directory = r\"deeplearning/data\"\n",
    "# #train\n",
    "# with open(directory+'/word2vec/equal_dist_train_wordvecs_v3', 'wb') as fp:\n",
    "#     pickle.dump(train_vecs_w2v, fp, protocol=2)\n",
    "\n",
    "# with open(directory+'/word2vec/equal_dist_trainw2v_label_v3', 'wb') as fp:\n",
    "#     pickle.dump(y_train, fp, protocol=2)\n",
    "\n",
    "# #val    \n",
    "# with open(directory+'/word2vec/equal_dist_val_wordvecs_v3', 'wb') as fp:\n",
    "#     pickle.dump(val_vecs_w2v, fp, protocol=2)\n",
    "    \n",
    "# with open(directory+'/word2vec/equal_dist_valw2v_label_v3', 'wb') as fp:\n",
    "#     pickle.dump(y_val, fp, protocol=2)\n",
    "    \n",
    "# #test   \n",
    "# with open(directory+'/word2vec/equal_dist_test_wordvecs_v3', 'wb') as fp:\n",
    "#     pickle.dump(test_vecs_w2v, fp, protocol=2)\n",
    "    \n",
    "# with open(directory+'/word2vec/equal_dist_testw2v_label_v3', 'wb') as fp:\n",
    "#     pickle.dump(y_test, fp, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "directory = r\"deeplearning/data\"\n",
    "with open (directory+'/word2vec/equal_dist_train_wordvecs_v3', 'rb') as fp:\n",
    "    train_vectors = pickle.load(fp)\n",
    "\n",
    "with open (directory+'/word2vec/equal_dist_trainw2v_label_v3', 'rb') as fp:\n",
    "    train_label = pickle.load(fp)\n",
    "\n",
    "with open (directory+'/word2vec/equal_dist_val_wordvecs_v3', 'rb') as fp:\n",
    "    val_vectors = pickle.load(fp)\n",
    "\n",
    "with open (directory+'/word2vec/equal_dist_valw2v_label_v3', 'rb') as fp:\n",
    "    val_label = pickle.load(fp)\n",
    "    \n",
    "with open (directory+'/word2vec/equal_dist_test_wordvecs_v3', 'rb') as fp:\n",
    "    test_vectors = pickle.load(fp)\n",
    "\n",
    "with open (directory+'/word2vec/equal_dist_testw2v_label_v3', 'rb') as fp:\n",
    "    test_label = pickle.load(fp)\n",
    "\n",
    "#out of sample vectors\n",
    "# with open (directory+'/word2vec/equal_dist_OOS_wordvectors_v3', 'rb') as fp:\n",
    "#     OOS_vectors = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "lr = SGDClassifier(loss='log', penalty='l1')\n",
    "lr.fit(train_vectors, train_label)\n",
    "\n",
    "print (lr.score(val_vectors, val_label))\n",
    "print (lr.score(test_vectors, test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_prob = lr.predict_proba(OOS_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_new_colwise=pd.concat([new_data,pd.DataFrame(lr_prob)],axis=1)\n",
    "df_new_colwise.sort_values(by=1, ascending=True).head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Latest Model word2vec\n",
    "from keras.optimizers import SGD, RMSprop, adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "import os\n",
    "os.environ['MKL_NUM_THREADS'] = '20'\n",
    "os.environ['GOTO_NUM_THREADS'] = '20'\n",
    "os.environ['OMP_NUM_THREADS'] = '20'\n",
    "os.environ['openmp'] = 'True'\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=200))\n",
    "# model.add(Dense(16, activation='relu') )\n",
    "# model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "opt = adam(lr=0.001)\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(train_vectors, train_label, nb_epoch=10, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(val_vectors, val_label, batch_size=128, verbose=2)\n",
    "print score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(test_vectors, test_label, batch_size=128, verbose=2)\n",
    "print score[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and load keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(directory+ \"/saved_models/equal_dist_keras_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(directory+ \"/saved_models/equal_dist_keras_model_weights.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # later...\n",
    "import pickle\n",
    "directory = r\"deeplearning/data\"\n",
    "# load json and create model\n",
    "json_file = open(directory+ \"/saved_models/equal_dist_keras_model.json\", 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(directory+ \"/saved_models/equal_dist_keras_model_weights.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OOS_predictions = loaded_model.predict_proba(OOS_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score ,confusion_matrix\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score, train_test_split \n",
    "\n",
    "###############################\n",
    "# df_risk_type = pd.read_excel('/home/hduser/Desktop/risk_category/training_risk_type_country.xls',sep=',')\n",
    "\n",
    "# df_risk_type = df_risk_type.fillna('')\n",
    "# df_risk_type['Text'] = df_risk_type['Title'] + \" \" + df_risk_type['Content'] + \" \" + df_risk_type['Text']\n",
    "\n",
    "# df_risk_type = df_risk_type.drop(['Link','Title','Content','Risk_Type'], axis=1)\n",
    "\n",
    "# print set(df_risk_type[\"Risk_Subtype\"].tolist())\n",
    "\n",
    "# ###############################\n",
    "# stop           = set(stopwords.words('english'))\n",
    "# tf             = TfidfVectorizer(use_idf=True,min_df = 20 ,stop_words=stop).fit(df_risk_type['Text'])\n",
    "\n",
    "# mails_tfidf    = tf.transform(df_risk_type['Text'])\n",
    "\n",
    "classifier     = SVC()\n",
    "\n",
    "parameters     = {'kernel':['rbf'], 'C':[1] ,'gamma' : [0.01]}\n",
    "\n",
    "clf= GridSearchC\n",
    "    classifier,  # pipeline from above\n",
    "    param_grid=parameters,  # parameters to tune via cross validation\n",
    "    refit=True,  # fit using all data, on the best detected classifier\n",
    "    n_jobs=4,  # number of cores to use for parallelization; -1 for \"all cores\"\n",
    "    scoring='accuracy',  # what score are we optimizing\n",
    "    cv=StratifiedKFold(train_label, n_folds=2),  # what type of cross validation to use\n",
    ")\n",
    "\n",
    "svm_detector = clf.fit(train_vectors,train_label)\n",
    "print svm_detector.grid_scores_\n",
    "print ()\n",
    "print(\"best score is : \"       ,svm_detector.best_score_)\n",
    "print(\"best parameters are : \" ,svm_detector.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(directory+'/saved_models/svm', 'wb') as fp:\n",
    "    pickle.dump(svm_detector, fp, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Negative Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_new_colwise = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_new_colwise=pd.concat([new_data,pd.DataFrame(OOS_predictions)],axis=1)\n",
    "df_new_colwise.sort_values(by=0).head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# df_new_colwise.plot.hist(df_new_colwise[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Positive Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_new_colwise.sort_values(by=0, ascending = False).head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_new_colwise.loc[df_new_colwise[0]<=0.10, 0] = \"negative\"\n",
    "df_new_colwise.loc[((df_new_colwise[0]>0.10) & (df_new_colwise[0]<=0.90)), 0] = \"neutral\"\n",
    "df_new_colwise.loc[((df_new_colwise[0]>0.90) & (df_new_colwise[0]<= 1.0)), 0] = \"positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_new_colwise[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_new_colwise[\"tweet_text_sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_new_colwise.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# remove punctuations\n",
    "def remove_stop_words(tokens):\n",
    "    tokens = \" \".join(tokens)\n",
    "    tokens = re.sub(\"[^a-zA-Z]\", \" \", tokens).split()\n",
    "    stops = set(stopwords.words(\"english\"))   \n",
    "    meaningful_words = [w for w in tokens if not w in stops]\n",
    "    meaningful_words = [w for w in meaningful_words if len(w) > 2]\n",
    "    return \" \".join(meaningful_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in df_new_colwise[\"tokens\"]:\n",
    "    print i\n",
    "    print remove_stop_words(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_new_colwise[\"cleaned_tokens\"] = df_new_colwise[\"tokens\"].apply(lambda x: remove_stop_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_new_colwise.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# used for getting the Topic Cloud\n",
    "def collocatedBigrams( total_text, tag = \"#amazon\"):\n",
    "    regex_token = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    full_words = []\n",
    "    for x in total_text:\n",
    "        full_words.extend(regex_token.tokenize(x))\n",
    "\n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(nltk.pos_tag(full_words))\n",
    "    if tag is not None:\n",
    "        if len(total_text) < 1000:\n",
    "            pass\n",
    "        else:\n",
    "            finder.apply_freq_filter(min_freq=5)\n",
    "    bi_final = finder.nbest(bigram_measures.chi_sq, 500)\n",
    "    imp_bigrams = [[x[0][0], x[1][0]] for x in bi_final if\n",
    "#                    x[0][1] in (\"JJ\", \"NN\", \"NNP\", \"NNS\") or x[1][1] in (\"NN\", \"NNP\", \"NNS\", \"JJ\")\n",
    "                    len(x[0][0]) >= 3 and len(x[1][0]) >= 3]\n",
    "    bi_score = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "    all_bigrams = {\" \".join([x[0][0][0], x[0][1][0]]) :x[1] for x in bi_score if len(x[0][0][0]) >= 3 and len(x[0][1][0]) >=3}\n",
    "    sorted_bigrams = {}\n",
    "    for bi in imp_bigrams:\n",
    "        if tag is None and \" \".join(bi) in all_bigrams.keys():\n",
    "            sorted_bigrams[\" \".join(bi)] = all_bigrams[\" \".join(bi)]\n",
    "        elif \" \".join(bi) in all_bigrams.keys() and tag not in \" \".join(bi):\n",
    "            sorted_bigrams[\" \".join(bi)] = all_bigrams[\" \".join(bi)]  \n",
    "    return sorted(sorted_bigrams.items(), key=lambda x: x[1], reverse = True)[:15]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "collocatedBigrams(list(df_new_colwise[df_new_colwise[\"tweet_text_sentiment\"] == 1][\"cleaned_tokens\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "collocatedBigrams(list(df_new_colwise[df_new_colwise[0] == \"positive\"][\"cleaned_tokens\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "collocatedBigrams(list(df_new_colwise[df_new_colwise[\"tweet_text_sentiment\"] == 0][\"cleaned_tokens\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "collocatedBigrams(list(df_new_colwise[df_new_colwise[0] == \"negative\"][\"cleaned_tokens\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "collocatedBigrams(list(df_new_colwise[df_new_colwise[\"tweet_text_sentiment\"] == 2][\"cleaned_tokens\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "collocatedBigrams(list(df_new_colwise[df_new_colwise[0] == \"neutral\"][\"cleaned_tokens\"]))+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_new_colwise[(df_new_colwise[0] == \"negative\") & (df_new_colwise.tweet_text.str.contains(\"goddess\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using the above udf functions\n",
    "# dat_corr = F.udf(data_correction)\n",
    "# brand_image = f.udf(brand_imagery, StringType())\n",
    "# consumer_theme_calc = f.udf(consumer_theme,StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Comparison of textblob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
