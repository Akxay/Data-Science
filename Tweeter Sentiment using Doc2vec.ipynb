{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import os\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "pd.options.display.max_colwidth = 500\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "import os\n",
    "os.environ['MKL_NUM_THREADS'] = '20'\n",
    "os.environ['GOTO_NUM_THREADS'] = '20'\n",
    "os.environ['OMP_NUM_THREADS'] = '20'\n",
    "os.environ['openmp'] = 'True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read file\n",
    "os.chdir(\"/deeplearning/data\")\n",
    "directory = os.getcwd()\n",
    "filepath=r\"/tweets_data/train.csv\"\n",
    "tweets_df = pd.read_csv(directory+filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_df.loc[tweets_df[\"sentiment\"]==4,\"sentiment\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tweets_df = pd.concat([tweets_df[1:6000],tweets_df[-6000:-1]],axis=0)\n",
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kinshuk/anaconda2/envs/python3/lib/python3.4/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 170 of the file /home/kinshuk/anaconda2/envs/python3/lib/python3.4/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"html.parser\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-107ccff3205b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mreview_text\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtweets_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"clean_text\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreview_to_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/kinshuk/anaconda2/envs/python3/lib/python3.4/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   2549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-107ccff3205b>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mreview_text\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtweets_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"clean_text\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreview_to_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-107ccff3205b>\u001b[0m in \u001b[0;36mreview_to_words\u001b[0;34m(raw_review)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreview_to_words\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mraw_review\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mreview_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_review\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mreview_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreview_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#REMOVE LINKS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mreview_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreview_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kinshuk/anaconda2/envs/python3/lib/python3.4/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m                     \u001b[0mmarkup_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"HTML\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m                 \u001b[0mcaller\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m                 \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0mline_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kinshuk/anaconda2/envs/python3/lib/python3.4/traceback.py\u001b[0m in \u001b[0;36mextract_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0moldest\u001b[0m \u001b[0mto\u001b[0m \u001b[0mnewest\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \"\"\"\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_stack_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m     \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kinshuk/anaconda2/envs/python3/lib/python3.4/traceback.py\u001b[0m in \u001b[0;36m_extract_tb_or_stack_iter\u001b[0;34m(curr, limit, extractor)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mlinecache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinecache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_globals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kinshuk/anaconda2/envs/python3/lib/python3.4/site-packages/IPython/core/compilerop.py\u001b[0m in \u001b[0;36mcheck_linecache_ipython\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;31m# Then, update back the cache with our data, so that tracebacks related\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;31m# to our compiled codes can be produced.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0mlinecache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinecache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ipython_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def review_to_words( raw_review ):\n",
    "    review_text = BeautifulSoup(raw_review).get_text()\n",
    "    review_text = review_text.lower()\n",
    "    #REMOVE LINKS\n",
    "    review_text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', review_text)\n",
    "    #remove at-mentions\n",
    "    review_text = review_text.replace(\".\",\"\")\n",
    "    review_text = re.sub(r'@.*?(?=\\s)', '', review_text)\n",
    "    review_text = review_text.replace(\"#\",\"\").split()\n",
    "#     review_text = re.sub(r'^\\+feedback', '', review_text)\n",
    "#     letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)     \n",
    "#     words = letters_only.lower().split()\n",
    "#    wordsList.extend(words)\n",
    "#     stops = set(stopwords.words(\"english\"))                  \n",
    "#     meaningful_words = [w for w in words if not w in stops]   \n",
    "    return( \" \".join( review_text ))      \n",
    "\n",
    "tweets_df[\"clean_text\"] = tweets_df[\"text\"].apply(lambda x: review_to_words(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = tweets_df[\"sentiment\"]\n",
    "X = tweets_df[\"clean_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read cleaned csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train = pd.read_csv(directory + \"/cleaned_data/x_train.csv\")\n",
    "x_train.columns = [\"id\", \"text\"]\n",
    "x_train[\"text\"] = x_train[\"text\"].astype('str')\n",
    "\n",
    "x_test = pd.read_csv(directory + \"/cleaned_data/x_test.csv\")\n",
    "x_test.columns = [\"id\", \"text\"]\n",
    "x_test[\"text\"] = x_train[\"text\"].astype('str')\n",
    "\n",
    "y_train = pd.read_csv(directory + \"/cleaned_data/y_train.csv\")\n",
    "y_test = pd.read_csv(directory + \"/cleaned_data/y_test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1598617</td>\n",
       "      <td>you're most welcome</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>620181</td>\n",
       "      <td>i wanted to see the tony awards i had to work</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76767</td>\n",
       "      <td>flooring picked out, that was easy! she had everything laid out for us now off to the amish craft store with mia and sue</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1394882</td>\n",
       "      <td>oh my shiznit he actually replied to you! how sweet haha</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>900229</td>\n",
       "      <td>of course!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  \\\n",
       "0  1598617   \n",
       "1   620181   \n",
       "2    76767   \n",
       "3  1394882   \n",
       "4   900229   \n",
       "\n",
       "                                                                                                                       text  \\\n",
       "0                                                                                                       you're most welcome   \n",
       "1                                                                             i wanted to see the tony awards i had to work   \n",
       "2  flooring picked out, that was easy! she had everything laid out for us now off to the amish craft store with mia and sue   \n",
       "3                                                                  oh my shiznit he actually replied to you! how sweet haha   \n",
       "4                                                                                                                of course!   \n",
       "\n",
       "   length  \n",
       "0       3  \n",
       "1      11  \n",
       "2      24  \n",
       "3      11  \n",
       "4       2  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[\"length\"] = x_train[\"text\"].apply(lambda x: len((x).split()))\n",
    "\n",
    "x_test[\"length\"] = x_test[\"text\"].apply(lambda x: len((x).split()))\n",
    "x_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1126285, 3)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[x_train[\"length\"] > 9].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(193611, 3)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[x_test[\"length\"] > 9].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating tagged docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=[\"you're\", 'most', 'welcome'], tags=[0]),\n",
       " TaggedDocument(words=['i', 'wanted', 'to', 'see', 'the', 'tony', 'awards', 'i', 'had', 'to', 'work'], tags=[1])]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_tagged_doc(sentences):\n",
    "    taggeddoc = []\n",
    "    for index,i in enumerate(sentences):\n",
    "        td = TaggedDocument(gensim.utils.to_unicode(str.encode(i)).split(),[index])\n",
    "        taggeddoc.append(td)\n",
    "    return taggeddoc\n",
    "taggeddoc = create_tagged_doc(x_train[\"text\"])\n",
    "taggeddoc[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch  1\n",
      "Epoch  1  took 13.403189329306285 minutes \n",
      "Starting Epoch  2\n",
      "Epoch  2  took 13.244980490207672 minutes \n",
      "Starting Epoch  3\n",
      "Epoch  3  took 13.133060296376547 minutes \n",
      "Starting Epoch  4\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import doc2vec\n",
    "import pickle\n",
    "import time\n",
    "from random import shuffle\n",
    "\n",
    "num_features = 5 #number of features/columns for the term-document matrix.\n",
    "\n",
    "min_word_count = 10 # ''' minimum word count: any word that does not occur at least this many times across all documents is ignored'''\n",
    "context = 3 # Context window size. The paper (http://arxiv.org/pdf/1405.4053v2.pdf) suggests 10 is the optimal\n",
    "\n",
    "downsampling = 1e-3  # ''' threshold for configuring which higher-frequency words are randomly downsampled; default is 0 (off), useful value is 1e-5 set the same as word2vec'''\n",
    "\n",
    "num_workers = 20  # Number of threads to run in parallel\n",
    "\n",
    "# build the Doc2vec model\n",
    "model = doc2vec.Doc2Vec(taggeddoc, dm=1, size=num_features,\n",
    "                        window=context, min_count=min_word_count,\n",
    "                        workers=num_workers,alpha=0.05,min_alpha=0.025)\n",
    "\n",
    "# model = doc2vec.Doc2Vec(taggeddoc, dm = 1, alpha=0.025, window=5, size= 10, min_alpha=0.025, min_count=1)\n",
    "\n",
    "# model.build_vocab(taggeddoc)\n",
    "\n",
    "# start training\n",
    "for epoch in range(1,5):    \n",
    "    print(\"Starting Epoch \",epoch)    \n",
    "    start_time = time.time()    \n",
    "    #Shuffle the tagged cleaned up reviews in each epoch\n",
    "    shuffle(taggeddoc)\n",
    "    \n",
    "    model.train(taggeddoc, total_examples=model.corpus_count, epochs=model.iter)\n",
    "#     model.alpha -= 0.02\n",
    "#     model.min_alpha = model.alpha\n",
    "    print(\"Epoch \",epoch,\" took %s minutes \" % ((time.time() - start_time)/60))\n",
    "\n",
    "gensim_vectors = []\n",
    "for item in x_train:\n",
    "    gensim_vectors.append(model.infer_vector(item.split()))\n",
    "\n",
    "#Save the trained model\t\n",
    "model.save(\"Doc2VecTaggedDocs_5_3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-987b3af79cdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaggeddoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0minferred_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaggeddoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minferred_vector\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdocid\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdocid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msims\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mranks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(taggeddoc)):\n",
    "    inferred_vector = model.infer_vector(taggeddoc[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "    second_ranks.append(sims[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 1,\n",
       "         1: 1,\n",
       "         2: 2,\n",
       "         6: 2,\n",
       "         8: 2,\n",
       "         9: 1,\n",
       "         11: 1,\n",
       "         14: 1,\n",
       "         16: 1,\n",
       "         17: 2,\n",
       "         18: 2,\n",
       "         19: 2,\n",
       "         20: 2,\n",
       "         23: 2,\n",
       "         24: 1,\n",
       "         25: 1,\n",
       "         26: 1,\n",
       "         28: 1,\n",
       "         29: 2,\n",
       "         30: 2,\n",
       "         31: 1,\n",
       "         33: 4,\n",
       "         34: 2,\n",
       "         35: 1,\n",
       "         36: 2,\n",
       "         38: 1,\n",
       "         39: 1,\n",
       "         40: 3,\n",
       "         43: 2,\n",
       "         46: 1,\n",
       "         47: 2,\n",
       "         48: 1,\n",
       "         49: 1,\n",
       "         57: 1,\n",
       "         59: 1,\n",
       "         60: 2,\n",
       "         61: 3,\n",
       "         62: 4,\n",
       "         63: 1,\n",
       "         66: 1,\n",
       "         67: 1,\n",
       "         68: 3,\n",
       "         70: 1,\n",
       "         73: 2,\n",
       "         74: 1,\n",
       "         76: 1,\n",
       "         78: 2,\n",
       "         79: 1,\n",
       "         80: 2,\n",
       "         83: 1,\n",
       "         84: 2,\n",
       "         85: 1,\n",
       "         86: 2,\n",
       "         90: 2,\n",
       "         91: 1,\n",
       "         92: 3,\n",
       "         93: 1,\n",
       "         94: 2,\n",
       "         96: 1,\n",
       "         97: 1,\n",
       "         98: 1,\n",
       "         99: 2,\n",
       "         102: 1,\n",
       "         103: 1,\n",
       "         104: 1,\n",
       "         105: 1,\n",
       "         107: 1,\n",
       "         110: 1,\n",
       "         111: 3,\n",
       "         113: 1,\n",
       "         114: 3,\n",
       "         115: 1,\n",
       "         116: 1,\n",
       "         117: 1,\n",
       "         118: 1,\n",
       "         120: 1,\n",
       "         121: 1,\n",
       "         122: 1,\n",
       "         123: 1,\n",
       "         126: 3,\n",
       "         130: 2,\n",
       "         131: 1,\n",
       "         132: 3,\n",
       "         135: 1,\n",
       "         138: 2,\n",
       "         143: 1,\n",
       "         147: 2,\n",
       "         148: 1,\n",
       "         149: 1,\n",
       "         150: 3,\n",
       "         151: 3,\n",
       "         152: 1,\n",
       "         153: 2,\n",
       "         154: 1,\n",
       "         155: 2,\n",
       "         157: 1,\n",
       "         159: 2,\n",
       "         161: 2,\n",
       "         163: 2,\n",
       "         165: 3,\n",
       "         166: 1,\n",
       "         168: 1,\n",
       "         171: 1,\n",
       "         173: 3,\n",
       "         174: 1,\n",
       "         176: 1,\n",
       "         179: 1,\n",
       "         180: 1,\n",
       "         181: 1,\n",
       "         182: 1,\n",
       "         183: 1,\n",
       "         184: 1,\n",
       "         185: 2,\n",
       "         187: 1,\n",
       "         191: 2,\n",
       "         194: 1,\n",
       "         195: 2,\n",
       "         197: 1,\n",
       "         198: 1,\n",
       "         199: 1,\n",
       "         200: 1,\n",
       "         201: 2,\n",
       "         203: 1,\n",
       "         204: 2,\n",
       "         209: 2,\n",
       "         210: 1,\n",
       "         211: 2,\n",
       "         212: 1,\n",
       "         216: 2,\n",
       "         217: 3,\n",
       "         218: 1,\n",
       "         220: 1,\n",
       "         223: 1,\n",
       "         226: 3,\n",
       "         227: 2,\n",
       "         228: 2,\n",
       "         229: 1,\n",
       "         230: 1,\n",
       "         231: 2,\n",
       "         234: 1,\n",
       "         235: 1,\n",
       "         238: 3,\n",
       "         241: 2,\n",
       "         243: 2,\n",
       "         249: 2,\n",
       "         250: 6,\n",
       "         251: 1,\n",
       "         252: 3,\n",
       "         254: 1,\n",
       "         256: 1,\n",
       "         257: 1,\n",
       "         258: 1,\n",
       "         261: 2,\n",
       "         262: 1,\n",
       "         266: 1,\n",
       "         267: 3,\n",
       "         268: 1,\n",
       "         269: 1,\n",
       "         271: 3,\n",
       "         272: 1,\n",
       "         273: 2,\n",
       "         275: 3,\n",
       "         278: 1,\n",
       "         279: 1,\n",
       "         282: 1,\n",
       "         283: 1,\n",
       "         286: 1,\n",
       "         288: 1,\n",
       "         289: 1,\n",
       "         291: 3,\n",
       "         293: 2,\n",
       "         296: 1,\n",
       "         297: 3,\n",
       "         298: 1,\n",
       "         299: 4,\n",
       "         301: 1,\n",
       "         302: 2,\n",
       "         305: 1,\n",
       "         309: 1,\n",
       "         312: 1,\n",
       "         313: 1,\n",
       "         317: 3,\n",
       "         318: 1,\n",
       "         319: 2,\n",
       "         321: 1,\n",
       "         322: 3,\n",
       "         323: 1,\n",
       "         324: 1,\n",
       "         325: 2,\n",
       "         327: 1,\n",
       "         328: 2,\n",
       "         329: 1,\n",
       "         330: 2,\n",
       "         332: 3,\n",
       "         333: 1,\n",
       "         334: 1,\n",
       "         335: 3,\n",
       "         336: 1,\n",
       "         338: 1,\n",
       "         339: 1,\n",
       "         342: 2,\n",
       "         343: 1,\n",
       "         345: 2,\n",
       "         346: 2,\n",
       "         348: 1,\n",
       "         350: 2,\n",
       "         351: 1,\n",
       "         352: 2,\n",
       "         353: 2,\n",
       "         356: 1,\n",
       "         358: 1,\n",
       "         359: 5,\n",
       "         360: 4,\n",
       "         362: 1,\n",
       "         367: 1,\n",
       "         368: 2,\n",
       "         369: 1,\n",
       "         372: 1,\n",
       "         373: 1,\n",
       "         375: 1,\n",
       "         377: 3,\n",
       "         380: 1,\n",
       "         381: 1,\n",
       "         382: 1,\n",
       "         383: 1,\n",
       "         384: 1,\n",
       "         385: 1,\n",
       "         386: 3,\n",
       "         387: 1,\n",
       "         389: 4,\n",
       "         390: 1,\n",
       "         391: 1,\n",
       "         392: 1,\n",
       "         396: 3,\n",
       "         398: 4,\n",
       "         400: 2,\n",
       "         401: 1,\n",
       "         402: 2,\n",
       "         403: 1,\n",
       "         404: 1,\n",
       "         405: 2,\n",
       "         406: 1,\n",
       "         407: 2,\n",
       "         409: 3,\n",
       "         411: 1,\n",
       "         413: 1,\n",
       "         414: 2,\n",
       "         415: 1,\n",
       "         416: 1,\n",
       "         417: 2,\n",
       "         418: 1,\n",
       "         419: 1,\n",
       "         420: 2,\n",
       "         421: 2,\n",
       "         422: 2,\n",
       "         424: 1,\n",
       "         425: 2,\n",
       "         426: 1,\n",
       "         429: 1,\n",
       "         430: 1,\n",
       "         431: 4,\n",
       "         432: 1,\n",
       "         433: 1,\n",
       "         435: 3,\n",
       "         436: 1,\n",
       "         437: 1,\n",
       "         438: 1,\n",
       "         439: 2,\n",
       "         440: 2,\n",
       "         443: 1,\n",
       "         446: 2,\n",
       "         447: 1,\n",
       "         448: 1,\n",
       "         449: 2,\n",
       "         450: 2,\n",
       "         452: 1,\n",
       "         453: 1,\n",
       "         454: 2,\n",
       "         456: 1,\n",
       "         458: 1,\n",
       "         459: 1,\n",
       "         460: 1,\n",
       "         461: 2,\n",
       "         463: 2,\n",
       "         464: 1,\n",
       "         465: 4,\n",
       "         467: 1,\n",
       "         469: 1,\n",
       "         473: 2,\n",
       "         474: 1,\n",
       "         475: 1,\n",
       "         476: 3,\n",
       "         477: 1,\n",
       "         479: 2,\n",
       "         480: 2,\n",
       "         481: 2,\n",
       "         482: 2,\n",
       "         484: 3,\n",
       "         485: 1,\n",
       "         488: 3,\n",
       "         489: 4,\n",
       "         490: 1,\n",
       "         494: 3,\n",
       "         497: 2,\n",
       "         499: 1,\n",
       "         500: 2,\n",
       "         501: 1,\n",
       "         502: 2,\n",
       "         504: 4,\n",
       "         505: 3,\n",
       "         506: 3,\n",
       "         507: 1,\n",
       "         508: 1,\n",
       "         509: 3,\n",
       "         512: 4,\n",
       "         517: 1,\n",
       "         518: 1,\n",
       "         519: 1,\n",
       "         521: 2,\n",
       "         523: 2,\n",
       "         524: 3,\n",
       "         526: 2,\n",
       "         527: 1,\n",
       "         531: 1,\n",
       "         535: 1,\n",
       "         537: 1,\n",
       "         540: 1,\n",
       "         541: 1,\n",
       "         544: 2,\n",
       "         545: 5,\n",
       "         546: 1,\n",
       "         549: 2,\n",
       "         550: 1,\n",
       "         551: 1,\n",
       "         552: 3,\n",
       "         554: 2,\n",
       "         555: 1,\n",
       "         556: 3,\n",
       "         557: 2,\n",
       "         559: 1,\n",
       "         562: 1,\n",
       "         563: 2,\n",
       "         567: 1,\n",
       "         569: 3,\n",
       "         575: 1,\n",
       "         578: 1,\n",
       "         579: 1,\n",
       "         581: 1,\n",
       "         582: 1,\n",
       "         585: 4,\n",
       "         586: 1,\n",
       "         587: 1,\n",
       "         591: 1,\n",
       "         594: 2,\n",
       "         597: 5,\n",
       "         600: 1,\n",
       "         601: 2,\n",
       "         604: 1,\n",
       "         605: 1,\n",
       "         606: 3,\n",
       "         607: 1,\n",
       "         608: 2,\n",
       "         610: 2,\n",
       "         611: 2,\n",
       "         612: 1,\n",
       "         613: 1,\n",
       "         614: 4,\n",
       "         617: 1,\n",
       "         618: 1,\n",
       "         619: 2,\n",
       "         620: 2,\n",
       "         621: 1,\n",
       "         625: 2,\n",
       "         627: 2,\n",
       "         631: 1,\n",
       "         632: 1,\n",
       "         633: 1,\n",
       "         634: 2,\n",
       "         636: 4,\n",
       "         637: 1,\n",
       "         638: 2,\n",
       "         639: 2,\n",
       "         641: 1,\n",
       "         642: 1,\n",
       "         643: 2,\n",
       "         649: 1,\n",
       "         650: 2,\n",
       "         652: 2,\n",
       "         653: 4,\n",
       "         655: 1,\n",
       "         658: 1,\n",
       "         659: 2,\n",
       "         660: 1,\n",
       "         661: 4,\n",
       "         662: 1,\n",
       "         663: 1,\n",
       "         664: 1,\n",
       "         665: 1,\n",
       "         666: 1,\n",
       "         667: 1,\n",
       "         668: 1,\n",
       "         669: 1,\n",
       "         671: 1,\n",
       "         672: 1,\n",
       "         674: 2,\n",
       "         675: 1,\n",
       "         676: 1,\n",
       "         677: 3,\n",
       "         678: 1,\n",
       "         680: 1,\n",
       "         683: 2,\n",
       "         686: 2,\n",
       "         687: 1,\n",
       "         688: 1,\n",
       "         689: 2,\n",
       "         692: 1,\n",
       "         693: 1,\n",
       "         694: 1,\n",
       "         695: 2,\n",
       "         696: 2,\n",
       "         699: 2,\n",
       "         700: 4,\n",
       "         702: 1,\n",
       "         703: 1,\n",
       "         704: 1,\n",
       "         705: 1,\n",
       "         708: 2,\n",
       "         711: 2,\n",
       "         713: 1,\n",
       "         716: 1,\n",
       "         718: 1,\n",
       "         719: 1,\n",
       "         720: 1,\n",
       "         724: 1,\n",
       "         725: 1,\n",
       "         728: 1,\n",
       "         731: 2,\n",
       "         733: 2,\n",
       "         737: 1,\n",
       "         740: 1,\n",
       "         743: 2,\n",
       "         744: 3,\n",
       "         746: 3,\n",
       "         748: 2,\n",
       "         749: 2,\n",
       "         750: 1,\n",
       "         751: 1,\n",
       "         752: 3,\n",
       "         754: 1,\n",
       "         757: 3,\n",
       "         759: 1,\n",
       "         762: 1,\n",
       "         763: 1,\n",
       "         765: 1,\n",
       "         766: 2,\n",
       "         767: 1,\n",
       "         768: 1,\n",
       "         769: 1,\n",
       "         770: 2,\n",
       "         771: 1,\n",
       "         773: 3,\n",
       "         774: 1,\n",
       "         775: 1,\n",
       "         777: 2,\n",
       "         780: 1,\n",
       "         783: 2,\n",
       "         786: 1,\n",
       "         787: 1,\n",
       "         788: 1,\n",
       "         789: 1,\n",
       "         792: 1,\n",
       "         793: 1,\n",
       "         794: 2,\n",
       "         800: 2,\n",
       "         801: 1,\n",
       "         802: 1,\n",
       "         803: 1,\n",
       "         807: 1,\n",
       "         808: 2,\n",
       "         809: 2,\n",
       "         811: 1,\n",
       "         812: 1,\n",
       "         813: 2,\n",
       "         814: 1,\n",
       "         815: 1,\n",
       "         816: 1,\n",
       "         817: 1,\n",
       "         819: 1,\n",
       "         820: 1,\n",
       "         822: 2,\n",
       "         823: 2,\n",
       "         824: 1,\n",
       "         825: 2,\n",
       "         827: 2,\n",
       "         828: 1,\n",
       "         830: 1,\n",
       "         831: 1,\n",
       "         832: 2,\n",
       "         834: 1,\n",
       "         836: 1,\n",
       "         837: 1,\n",
       "         840: 1,\n",
       "         841: 1,\n",
       "         842: 3,\n",
       "         843: 2,\n",
       "         844: 1,\n",
       "         846: 2,\n",
       "         847: 1,\n",
       "         849: 3,\n",
       "         852: 1,\n",
       "         853: 3,\n",
       "         854: 1,\n",
       "         855: 1,\n",
       "         856: 3,\n",
       "         857: 1,\n",
       "         858: 1,\n",
       "         859: 1,\n",
       "         860: 1,\n",
       "         861: 2,\n",
       "         863: 1,\n",
       "         864: 1,\n",
       "         865: 2,\n",
       "         866: 1,\n",
       "         868: 1,\n",
       "         869: 2,\n",
       "         870: 1,\n",
       "         871: 1,\n",
       "         873: 3,\n",
       "         877: 1,\n",
       "         878: 1,\n",
       "         879: 1,\n",
       "         880: 1,\n",
       "         881: 2,\n",
       "         882: 2,\n",
       "         883: 1,\n",
       "         884: 3,\n",
       "         885: 1,\n",
       "         891: 1,\n",
       "         892: 1,\n",
       "         896: 2,\n",
       "         899: 1,\n",
       "         900: 2,\n",
       "         901: 1,\n",
       "         902: 1,\n",
       "         905: 2,\n",
       "         907: 1,\n",
       "         908: 2,\n",
       "         909: 2,\n",
       "         912: 2,\n",
       "         913: 1,\n",
       "         914: 2,\n",
       "         915: 2,\n",
       "         916: 3,\n",
       "         917: 2,\n",
       "         918: 1,\n",
       "         923: 2,\n",
       "         924: 2,\n",
       "         925: 2,\n",
       "         926: 2,\n",
       "         927: 3,\n",
       "         928: 1,\n",
       "         929: 1,\n",
       "         930: 1,\n",
       "         931: 1,\n",
       "         932: 4,\n",
       "         933: 1,\n",
       "         934: 2,\n",
       "         935: 1,\n",
       "         937: 1,\n",
       "         939: 2,\n",
       "         941: 2,\n",
       "         942: 1,\n",
       "         943: 1,\n",
       "         944: 1,\n",
       "         946: 1,\n",
       "         947: 2,\n",
       "         948: 1,\n",
       "         949: 1,\n",
       "         950: 1,\n",
       "         951: 2,\n",
       "         952: 2,\n",
       "         953: 1,\n",
       "         955: 1,\n",
       "         956: 2,\n",
       "         957: 1,\n",
       "         958: 2,\n",
       "         959: 3,\n",
       "         961: 1,\n",
       "         962: 1,\n",
       "         963: 1,\n",
       "         964: 1,\n",
       "         965: 1,\n",
       "         968: 2,\n",
       "         971: 2,\n",
       "         972: 1,\n",
       "         974: 1,\n",
       "         975: 2,\n",
       "         978: 1,\n",
       "         979: 2,\n",
       "         980: 2,\n",
       "         983: 1,\n",
       "         984: 1,\n",
       "         985: 1,\n",
       "         986: 2,\n",
       "         987: 1,\n",
       "         989: 2,\n",
       "         990: 2,\n",
       "         991: 1,\n",
       "         993: 1,\n",
       "         994: 1,\n",
       "         995: 1,\n",
       "         997: 2,\n",
       "         999: 1,\n",
       "         1000: 1,\n",
       "         1001: 3,\n",
       "         1002: 1,\n",
       "         1003: 1,\n",
       "         1004: 1,\n",
       "         1006: 2,\n",
       "         1008: 1,\n",
       "         1010: 1,\n",
       "         1013: 2,\n",
       "         1014: 1,\n",
       "         1016: 2,\n",
       "         1020: 2,\n",
       "         1021: 2,\n",
       "         1023: 1,\n",
       "         1026: 1,\n",
       "         1028: 2,\n",
       "         1029: 2,\n",
       "         1030: 2,\n",
       "         1032: 2,\n",
       "         1033: 3,\n",
       "         1034: 2,\n",
       "         1036: 1,\n",
       "         1038: 3,\n",
       "         1040: 2,\n",
       "         1042: 1,\n",
       "         1043: 1,\n",
       "         1044: 1,\n",
       "         1045: 1,\n",
       "         1046: 1,\n",
       "         1049: 1,\n",
       "         1050: 1,\n",
       "         1052: 1,\n",
       "         1053: 1,\n",
       "         1055: 1,\n",
       "         1058: 1,\n",
       "         1062: 1,\n",
       "         1064: 3,\n",
       "         1065: 1,\n",
       "         1066: 1,\n",
       "         1068: 2,\n",
       "         1069: 3,\n",
       "         1070: 3,\n",
       "         1074: 3,\n",
       "         1076: 1,\n",
       "         1079: 1,\n",
       "         1080: 2,\n",
       "         1081: 1,\n",
       "         1086: 2,\n",
       "         1087: 1,\n",
       "         1088: 1,\n",
       "         1089: 2,\n",
       "         1090: 1,\n",
       "         1091: 1,\n",
       "         1092: 2,\n",
       "         1093: 1,\n",
       "         1094: 1,\n",
       "         1096: 1,\n",
       "         1097: 2,\n",
       "         1100: 1,\n",
       "         1101: 2,\n",
       "         1103: 1,\n",
       "         1106: 2,\n",
       "         1108: 2,\n",
       "         1110: 1,\n",
       "         1114: 2,\n",
       "         1115: 1,\n",
       "         1117: 1,\n",
       "         1118: 1,\n",
       "         1120: 3,\n",
       "         1121: 1,\n",
       "         1122: 1,\n",
       "         1123: 1,\n",
       "         1125: 1,\n",
       "         1126: 2,\n",
       "         1129: 1,\n",
       "         1130: 1,\n",
       "         1132: 2,\n",
       "         1133: 1,\n",
       "         1134: 2,\n",
       "         1135: 1,\n",
       "         1138: 1,\n",
       "         1140: 3,\n",
       "         1141: 1,\n",
       "         1144: 1,\n",
       "         1147: 2,\n",
       "         1149: 5,\n",
       "         1150: 2,\n",
       "         1151: 2,\n",
       "         1153: 1,\n",
       "         1154: 1,\n",
       "         1155: 1,\n",
       "         1156: 3,\n",
       "         1157: 1,\n",
       "         1158: 1,\n",
       "         1159: 3,\n",
       "         1161: 2,\n",
       "         1162: 1,\n",
       "         1163: 3,\n",
       "         1165: 2,\n",
       "         1166: 1,\n",
       "         1168: 2,\n",
       "         1172: 2,\n",
       "         1173: 2,\n",
       "         1175: 2,\n",
       "         1176: 1,\n",
       "         1179: 1,\n",
       "         1180: 1,\n",
       "         1182: 2,\n",
       "         1183: 1,\n",
       "         1184: 1,\n",
       "         1185: 1,\n",
       "         1186: 1,\n",
       "         1188: 2,\n",
       "         1189: 1,\n",
       "         1190: 1,\n",
       "         1192: 3,\n",
       "         1193: 4,\n",
       "         1194: 1,\n",
       "         1196: 1,\n",
       "         1197: 1,\n",
       "         1198: 1,\n",
       "         1199: 1,\n",
       "         1201: 1,\n",
       "         1202: 1,\n",
       "         1203: 1,\n",
       "         1204: 2,\n",
       "         1205: 1,\n",
       "         1207: 1,\n",
       "         1209: 1,\n",
       "         1210: 2,\n",
       "         1211: 2,\n",
       "         1212: 1,\n",
       "         1213: 1,\n",
       "         1216: 2,\n",
       "         1218: 4,\n",
       "         1219: 1,\n",
       "         1220: 1,\n",
       "         1221: 1,\n",
       "         1222: 1,\n",
       "         1223: 1,\n",
       "         1224: 3,\n",
       "         1226: 1,\n",
       "         1227: 1,\n",
       "         1228: 1,\n",
       "         1229: 1,\n",
       "         1230: 1,\n",
       "         1232: 3,\n",
       "         1233: 3,\n",
       "         1234: 2,\n",
       "         1237: 3,\n",
       "         1238: 1,\n",
       "         1239: 2,\n",
       "         1242: 1,\n",
       "         1243: 1,\n",
       "         1245: 1,\n",
       "         1246: 1,\n",
       "         1248: 1,\n",
       "         1251: 1,\n",
       "         1252: 1,\n",
       "         1253: 1,\n",
       "         1254: 1,\n",
       "         1255: 2,\n",
       "         1257: 1,\n",
       "         1260: 1,\n",
       "         1261: 2,\n",
       "         1263: 5,\n",
       "         1264: 2,\n",
       "         1266: 2,\n",
       "         1267: 1,\n",
       "         1268: 1,\n",
       "         1269: 1,\n",
       "         1270: 2,\n",
       "         1275: 1,\n",
       "         1280: 2,\n",
       "         1282: 1,\n",
       "         1284: 2,\n",
       "         1285: 2,\n",
       "         1287: 1,\n",
       "         1289: 2,\n",
       "         1290: 1,\n",
       "         1291: 1,\n",
       "         1292: 1,\n",
       "         1293: 1,\n",
       "         1294: 1,\n",
       "         1295: 1,\n",
       "         1296: 3,\n",
       "         1297: 2,\n",
       "         1298: 2,\n",
       "         1300: 5,\n",
       "         1301: 1,\n",
       "         1302: 2,\n",
       "         1305: 1,\n",
       "         1308: 2,\n",
       "         1310: 1,\n",
       "         1311: 3,\n",
       "         1314: 3,\n",
       "         1315: 1,\n",
       "         1317: 1,\n",
       "         1320: 1,\n",
       "         1321: 4,\n",
       "         1322: 3,\n",
       "         1324: 1,\n",
       "         1325: 2,\n",
       "         1327: 1,\n",
       "         1328: 2,\n",
       "         1329: 2,\n",
       "         1331: 1,\n",
       "         1332: 1,\n",
       "         1333: 1,\n",
       "         1334: 1,\n",
       "         1335: 1,\n",
       "         1337: 2,\n",
       "         1339: 2,\n",
       "         1340: 2,\n",
       "         1341: 1,\n",
       "         1342: 2,\n",
       "         1344: 2,\n",
       "         1345: 1,\n",
       "         1346: 1,\n",
       "         1348: 1,\n",
       "         1349: 1,\n",
       "         1352: 1,\n",
       "         1356: 2,\n",
       "         1357: 1,\n",
       "         1358: 1,\n",
       "         1359: 1,\n",
       "         1360: 2,\n",
       "         1362: 2,\n",
       "         1363: 1,\n",
       "         1365: 3,\n",
       "         1367: 1,\n",
       "         1368: 3,\n",
       "         1370: 1,\n",
       "         1371: 3,\n",
       "         1373: 1,\n",
       "         1375: 1,\n",
       "         1376: 1,\n",
       "         1378: 1,\n",
       "         1379: 1,\n",
       "         1382: 1,\n",
       "         1385: 1,\n",
       "         1386: 1,\n",
       "         1389: 2,\n",
       "         1391: 1,\n",
       "         1392: 2,\n",
       "         1393: 5,\n",
       "         1394: 1,\n",
       "         1396: 1,\n",
       "         1398: 3,\n",
       "         1401: 2,\n",
       "         1403: 1,\n",
       "         1405: 2,\n",
       "         1406: 1,\n",
       "         1407: 1,\n",
       "         1411: 2,\n",
       "         1412: 1,\n",
       "         1413: 1,\n",
       "         1415: 1,\n",
       "         1417: 2,\n",
       "         1418: 1,\n",
       "         1421: 1,\n",
       "         1422: 2,\n",
       "         1423: 3,\n",
       "         1424: 1,\n",
       "         1426: 2,\n",
       "         1427: 1,\n",
       "         1428: 1,\n",
       "         1431: 1,\n",
       "         1433: 1,\n",
       "         1434: 3,\n",
       "         1436: 4,\n",
       "         1438: 1,\n",
       "         1439: 1,\n",
       "         1440: 1,\n",
       "         1443: 3,\n",
       "         1444: 5,\n",
       "         1445: 1,\n",
       "         1447: 1,\n",
       "         1448: 1,\n",
       "         1450: 2,\n",
       "         1454: 1,\n",
       "         1455: 1,\n",
       "         1456: 2,\n",
       "         1457: 1,\n",
       "         1458: 2,\n",
       "         1459: 2,\n",
       "         1460: 3,\n",
       "         1461: 1,\n",
       "         1462: 1,\n",
       "         1463: 1,\n",
       "         1464: 1,\n",
       "         1468: 4,\n",
       "         1470: 2,\n",
       "         1471: 1,\n",
       "         1472: 1,\n",
       "         1473: 1,\n",
       "         1474: 1,\n",
       "         1475: 2,\n",
       "         1476: 2,\n",
       "         1477: 1,\n",
       "         1478: 3,\n",
       "         1479: 1,\n",
       "         1480: 1,\n",
       "         1490: 1,\n",
       "         1493: 4,\n",
       "         1495: 1,\n",
       "         1496: 2,\n",
       "         1497: 1,\n",
       "         1498: 3,\n",
       "         1500: 1,\n",
       "         1501: 3,\n",
       "         1503: 2,\n",
       "         1504: 1,\n",
       "         1505: 1,\n",
       "         1506: 1,\n",
       "         1507: 4,\n",
       "         1508: 2,\n",
       "         1511: 2,\n",
       "         1515: 2,\n",
       "         1516: 1,\n",
       "         1520: 2,\n",
       "         1521: 1,\n",
       "         1524: 4,\n",
       "         1526: 1,\n",
       "         1527: 3,\n",
       "         1529: 2,\n",
       "         1531: 1,\n",
       "         1532: 1,\n",
       "         1533: 1,\n",
       "         1535: 1,\n",
       "         1537: 1,\n",
       "         1538: 2,\n",
       "         1539: 1,\n",
       "         1542: 2,\n",
       "         1543: 1,\n",
       "         1544: 2,\n",
       "         1545: 1,\n",
       "         1547: 1,\n",
       "         1550: 3,\n",
       "         1555: 1,\n",
       "         1557: 1,\n",
       "         1559: 1,\n",
       "         1560: 1,\n",
       "         1562: 1,\n",
       "         1563: 1,\n",
       "         1564: 2,\n",
       "         1565: 1,\n",
       "         1567: 3,\n",
       "         1569: 1,\n",
       "         1570: 1,\n",
       "         1572: 2,\n",
       "         1573: 2,\n",
       "         1574: 3,\n",
       "         1575: 3,\n",
       "         1577: 3,\n",
       "         1580: 1,\n",
       "         1582: 2,\n",
       "         1583: 1,\n",
       "         1584: 1,\n",
       "         1586: 2,\n",
       "         1587: 2,\n",
       "         1588: 1,\n",
       "         1589: 2,\n",
       "         1590: 1,\n",
       "         1591: 2,\n",
       "         1592: 2,\n",
       "         1593: 2,\n",
       "         1595: 1,\n",
       "         1598: 4,\n",
       "         1599: 1,\n",
       "         1600: 1,\n",
       "         1601: 1,\n",
       "         1602: 1,\n",
       "         1603: 1,\n",
       "         1604: 4,\n",
       "         1605: 1,\n",
       "         1606: 1,\n",
       "         1610: 1,\n",
       "         1612: 1,\n",
       "         1613: 3,\n",
       "         1615: 2,\n",
       "         1616: 2,\n",
       "         1617: 1,\n",
       "         1618: 1,\n",
       "         1619: 1,\n",
       "         1621: 1,\n",
       "         1623: 1,\n",
       "         ...})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's count how each document ranks with respect to the training corpus\n",
    "import collections\n",
    "collections.Counter(ranks)  # Results vary due to random seeding and very small corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-a0c16f27cb4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# This is great and not entirely surprising. We can take a look at an example:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# This is great and not entirely surprising. We can take a look at an example:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Document ({}): {}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaggeddoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MOST'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'MEDIAN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'LEAST'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msims\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'doc_id' is not defined"
     ]
    }
   ],
   "source": [
    "### Basically, greater than 95% of the inferred documents are found to be most similar to itself and about 5% of the time it is mistakenly most similar to another document. the checking of an inferred-vector against a training-vector is a sort of 'sanity check' as to whether the model is behaving in a usefully consistent manner, though not a real 'accuracy' value.\n",
    "# This is great and not entirely surprising. We can take a look at an example:\n",
    "# This is great and not entirely surprising. We can take a look at an example:\n",
    "print('Document ({}): {}\\n'.format(doc_id, ' '.join(taggeddoc[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: %s\\n' % (label, sims[index], ' '.join(taggeddoc[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Document (874564): maths exam\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'second_ranks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-8b25c2d65fd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Compare and print the most/median/least similar documents from the train corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train Document ({}): {}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaggeddoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msim_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msecond_ranks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Similar Document {}: {}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaggeddoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msim_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'second_ranks' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(taggeddoc))\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "print('Train Document ({}): {}\\n'.format(doc_id, ' '.join(taggeddoc[doc_id].words)))\n",
    "sim_id = second_ranks[doc_id]\n",
    "print('Similar Document {}: {}\\n'.format(sim_id, ' '.join(taggeddoc[sim_id[0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get test data in shape\n",
    "test_corpus = x_test.apply(lambda x :x.split()).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Document (70049): oh, a chicken hat, of course! i don't know why i didn't think of that\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d300,n5,w8,mc20,s0.001):\n",
      "\n",
      "MOST (776027, 0.5999734401702881): sad i might not go meet this weekend u dont know how much i love them ughh my mom said she doesnt know the road of\n",
      "\n",
      "MEDIAN (534258, 0.2017820179462433): i was on campus for a grand total of 20 minutes today i love short days\n",
      "\n",
      "LEAST (669181, -0.5480166673660278): well they certainly don't know you then\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(test_corpus))\n",
    "inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
    "sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "print('Test Document ({}): {}\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: %s\\n' % (label, sims[index], ' '.join(taggeddoc[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get vectors for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gensim_vectors_test = []\n",
    "for item in x_test:\n",
    "    gensim_vectors_test.append(model.infer_vector(item.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(directory+'/embeddings/train_vectors.txt', 'wb') as fp:\n",
    "    pickle.dump(gensim_vectors, fp, protocol=2)\n",
    "\n",
    "with open(directory+'/embeddings/train_label.txt', 'wb') as fp:\n",
    "    pickle.dump(y_train, fp, protocol=2)\n",
    "\n",
    "with open(directory+'/embeddings/test_vectors.txt', 'wb') as fp:\n",
    "    pickle.dump(gensim_vectors_test, fp, protocol=2)\n",
    "    \n",
    "with open(directory+'/embeddings/test_label.txt', 'wb') as fp:\n",
    "    pickle.dump(y_test, fp, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Save cleaned file\n",
    "x_train.to_csv(directory+\"/cleaned_data/x_train.csv\")\n",
    "x_test.to_csv(directory+\"/cleaned_data/x_test.csv\")\n",
    "y_train.to_csv(directory+\"/cleaned_data/y_train.csv\")\n",
    "y_test.to_csv(directory+\"/cleaned_data/y_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar docs and all random stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-06473cc92a7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0_lockf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# access individual word vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# returns a 1 * # of features numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# print(model[\"good\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "print(type(model.syn0_lockf.shape))\n",
    "\n",
    "# access individual word vector\n",
    "# returns a 1 * # of features numpy array\n",
    "# print(model[\"good\"])\n",
    "\n",
    "# doesnt_match function tries to deduce which word in a set is most\n",
    "# dissimilar from the others\n",
    "# print(model.doesnt_match(\"\".split()) + '\\n')\n",
    "\n",
    "# most_similar(): returns the score of the most similar words based on the criteria\n",
    "# Find the top-N most similar words. Positive words contribute positively towards the\n",
    "# similarity, negative words negatively.\n",
    "\n",
    "print(\"most similar:\")\n",
    "# print(model.most_similar(positive=['hate', 'bad'], negative=['hate'], topn=10))\n",
    "print(model.most_similar(positive=['bad']))\n",
    "\n",
    "# print(model.docvecs.most_similar(str(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "#loading the model\n",
    "d2v_model = gensim.models.doc2vec.Doc2Vec.load(directory+\"/saved_models/Doc2VecTaggedDocs_300_8\")\n",
    "#start testing\n",
    "# printing the vector of document at index 1 in docLabels\n",
    "docvec = d2v_model.docvecs[2]\n",
    "print (len(docvec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Load vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open (directory+'/embeddings/train_vectors.txt', 'rb') as fp:\n",
    "    train_vectors = pickle.load(fp)\n",
    "\n",
    "with open (directory+'/embeddings/train_label.txt', 'rb') as fp:\n",
    "    train_label = pickle.load(fp)\n",
    "\n",
    "with open (directory+'/embeddings/test_vectors.txt', 'rb') as fp:\n",
    "    test_vectors = pickle.load(fp)\n",
    "\n",
    "with open (directory+'/embeddings/test_label.txt', 'rb') as fp:\n",
    "    test_label = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(directory+'/embeddings/train_vectors.txt', 'wb') as fp:\n",
    "    pickle.dump(train_vectors, fp, protocol=2)\n",
    "\n",
    "with open(directory+'/embeddings/train_label.txt', 'wb') as fp:\n",
    "    pickle.dump(train_label, fp, protocol=2)\n",
    "\n",
    "with open(directory+'/embeddings/test_vectors.txt', 'wb') as fp:\n",
    "    pickle.dump(test_vectors, fp, protocol=2)\n",
    "    \n",
    "with open(directory+'/embeddings/test_label.txt', 'wb') as fp:\n",
    "    pickle.dump(test_label, fp, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use embeddings from above to build a sentiment classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.504246875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "lr = SGDClassifier(loss='log', penalty='l1')\n",
    "lr.fit(train_vectors, train_label)\n",
    "\n",
    "print (lr.score(test_vectors, test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAIABJREFUeJzt3Xt8zvX/x/HH2+yAzWkHxqxh2GzO\nY0hIESIilUKhJKeU/OibUkmOnYQcS3QgKYccU045tjnOmTlsY3Y+n65t798fkyS1Ybs+13a97reb\nW7uu63Ndn6dP89xn78/n8/4orTVCCCFKvlJGBxBCCGEeUvhCCGElpPCFEMJKSOELIYSVkMIXQggr\nIYUvhBBWQgpfCCGshBS+EEJYCSl8IYSwEqWNWrGLi4v28vIyavVCCFEsBQcHx2itXe/mvYYVvpeX\nF0FBQUatXgghiiWl1KW7fa8M6QghhJWQwhdCCCshhS+EEFZCCl8IIayEFL4QQliJfAtfKfWFUipK\nKRXyL68rpdQspdQ5pdRRpVTTwo8phBDiXhVkD38J0Pk/Xu8C1Ln+Zwjw+b3HEkIIUdjyLXyt9U4g\n7j8W6QEs1Xn2ARWVUu6FFVAIIUQeU07uPb2/MC68qg6E3fQ4/PpzVwvhs4UQosTKydXEpGQSnZxJ\nVHIGUUl/fp1JTEomcalZJKSZiEvNJOLwDpJO7bmn9Zn1Slul1BDyhn3w9PQ056qFEMLskjJMXElI\nJzwunYiEdMLj07iSkEFEQjpXE9OJSckiJ1f/430Vytji6mRP5bJ2VM5N4PxPM7l6cBfVatUj9R7y\nFEbhRwA1bnrscf25f9BaLwAWAAQEBPzzbymEEMVIVnYuVxLSuRibyrmoFMLi0giPT+dKYgYR8Wkk\nZWT/bXn70qWoVrEM1SuWoV1dV9ycHKhSwYEqTva4ONlTpbwDzuXscLC1AUBrTUBAAJGnT/Phhx8y\natQobG1t7zpvYRT+WmCEUmo5EAgkaq1lOEcIUSJorYlKzuR8dF6hn4tK4eTVZC7EpHI1MZ2bd9Ad\n7UvjUSmv0APuq0SNymVuFLxHpbI4l7OjVCmV7zr37NlDgwYNcHJyYtGiRbi4uFCjRo1835effAtf\nKfUd0B5wUUqFAxMBWwCt9TxgA9AVOAekAQPvOZUQQphZTq7mSkI6Z6OSOReVwrmoFM5Hp3I+OoWE\nNNON5exKl6JeFSeae1WiRuXqeFYuy33O5ajtWg5nR/t7yhAbG8v48eNZtGgREydO5J133qFJkyb3\n+le7Id/C11r3zed1DQwvtERCCFHEUjOzORWZxMmryZy4msTxiERORSaTmf3XWTAujvbUdi1HF/+q\n+FQtT21XRzwrl6V6pTLYFGAv/U5orVm6dCmvv/468fHxjB07lrFjxxbqOsDA6ZGFEKKoaa2JTMog\nJCKJ09cL/lhEImHxaejrQzFODqXxr1aBfi3vo46bI95ujtRxc6JC2bsfK79T48aNY8aMGbRu3Zp5\n8+bRoEGDIlmPFL4QokT4s9xPXEniSFgCxyISORaRSExK1o1lalQug3+1CvRu6oGvuxP1q5WnesUy\nKFW4e+wFkZ6eTmpqKi4uLgwePJg6deowePBgSpUquhlvpPCFEMVSSmY2R8ISOHElieBL8QRfjic6\nORMAm1KKOm6OtKvrRoPq5WngUYG6VZxwcjDfXvt/2bRpE8OHD6dx48asWrWKevXqUa9evSJfrxS+\nEMLimXJyOXMtmYOX4jkclkhIRCLnolNunMPuUakMbbxdaORRAf/qFfBxL4+jveXV25UrVxg9ejQr\nV66kXr16jBgxwqzrt7wtIoSwekkZJo6EJRB8KZ6952M5Gp5IuikHABdHOxpUr0AnvyoEeFXGv1r5\nez47xhx+/fVXHn/8cbKyspg0aRJjx47F3t68uaXwhRCG0lpzNiqF4EvxHItI5NDlBE5FJqE1KAV+\n1crzVPMaNPGsSJMaeee2GzHmfrdMJhO2trY0atSIrl278v777+Pt7W1IFqW1MRe8BgQEaLmJuRDW\n6WJMKgcuxvH72Rj2nI+5cWC1vENp/KtXILCmM008K9LYsyLlLWTc/U4lJSXx1ltvsX//fnbv3o2N\njU2hfK5SKlhrHXA375U9fCFEkYtMzGD3uRiCLsWx/0IcodF5M8K4ONrRxtuF1t4uNPeqjJdz2WK1\n9347Wmt++OEHXnnlFSIjIxk2bBiZmZmULVvW6GhS+EKIwpeUYeJAaBy/n4vh93MxnItKAfL24Jt4\nVuK5Vl60qu1MHTfHYl/wN4uOjua5555j48aNNGnShDVr1tC8eXOjY90ghS+EuGcxKZnsD43jj4t5\nf05czRuDd7AtRYuazjwZ4EHr2i7Udy9foLlkiqvy5csTExPDJ598wvDhwyld2rIq1rLSCCGKhQxT\nDkEX49l1NpodZ6I5FZkMQBlbGxrVqMCoDnUIrFmZZl6VsC9dOGPXlmrnzp1MnjyZVatW4ejoyL59\n+4r04ql7IYUvhMiX1poLMansOhvDr6ei2B8aS2Z2LrY2imb3VWLsI/VoXdsZ/+oVsLWxzLIrbDEx\nMYwdO5YlS5bg5eXFxYsX8ff3t9iyByl8IcS/SMvKZv+FOLadimLb6SjC4tIB8HIuyzOBnjxQx4WW\ntZwpa2ddNaK15ssvv2Ts2LEkJSXxxhtvMGHCBIs4KJsf6/o/JYT4T4npJrafjmL90atsPxNNVnYu\nZWxtaFXbmSEP1KJtXVfucy5ndEzDff3119SvX5958+bh5+dndJwCk8IXwspdTUxn64lr/Hoqit3n\nYjDlaFyd7HmmhScP+rgRWLPyjTswWau0tDQ++OADhg4dioeHB6tWraJChQoWPXxzO1L4QlgZrTWn\nIpPZfDySbaeiOBKeCIBn5bI839qLzv5VaVKjUok+m+ZObNiwgeHDh3Px4kWqV6/Oyy+/TKVKlYyO\ndVek8IWwAjm5mj8uxrEpJJKtJ68RHp+OUtC4RkVe71SXLg3cqe3qaHRMixIeHs7o0aNZtWoVvr6+\n7Nixg7Zt2xod655I4QtRQmVm57DrTAy/nrrGluPXiE3Nwq50KdrWcWFou9p08a9aLCYdM8rkyZNZ\nv349H3zwAWPGjMHOzs7oSPdM5tIRogTJys5l9/kY1hyKYOvJKFIysylnZ8ODPm508XenXT1Xi5w2\n2FIcOHCAMmXK0KBBA2JjY0lMTKRWrVpGx/obmUtHCCuWnZPLgYtxrD96lfXHrpKQZsLJoTTdGrrT\n2b8qrWo7l/iLn+5VYmIi//vf//j888/p1q0ba9euxdnZGWdnZ6OjFSopfCGKIa01By/Hs+bwFTYc\niyQmJZOydjZ08HGjZ+PqPFDXRUq+ALTWrFixgldffZWoqChGjhzJpEmTjI5VZKTwhSgmtNYci0hk\n/bGrbDh2lbC4dOxLl6KDjxuPNapGu3quVncR1L36+uuvGTBgAAEBAfz88880a9bM6EhFSr47hLBw\nF2NS2RgSyZrDEZyKTKZ0KUWbOi6M7FCHrg3cZUz+DmVmZhIaGoqvry9PPvkk2dnZDBgwoNDmq7dk\n8p0ihAXKzM5h7eErrPgjjKBL8QA0qlGRyY/709XfnUrliv8ZI0bYtm0bL7/8MmlpaZw9exZ7e3sG\nDhxodCyzkcIXwoKExaXxzf7LrAwKIzY1i1qu5Xijiw+PNnTHo5Llz9ViqaKionj99ddZtmwZtWrV\nYsGCBWa/n6wlkMIXwmDpWTlsOHaVH4LD2XchFgU87FuFfi3v44E6LiXqBiFGOHfuHC1atCAlJYU3\n33yTN998kzJlyhgdyxBS+EIYIO8smwR+OhTOmsNXSM7IxrNyWUY/VJfezarL3nwhSEpKonz58tSu\nXZvBgwczaNAgfH19jY5lKCl8IcwoKjmDtYev8H1QGGeupWBfuhSd/avSt4UngTUry958IUhNTeW9\n995j4cKFHD16FA8PD2bMmGF0LIsghS9EEcvN1ew8G83yA2H8euoaphxNQ48KTO3VgEcbuuPkYGt0\nxBJj3bp1jBgxgsuXLzN48OBiMUe9OUnhC1FEkjNMrPgjjK/2XiQsLh3ncnb0b+nFM4GeeLvJRGWF\nKTs7myeffJKffvoJPz8/du3aRZs2bYyOZXGk8IUoZGFxaSzaFcqPByNIzsymuVclXu9Uj87+VeXq\n10KmtUYpRenSpXF3d2fq1Km8+uqrJWKis6IghS9EIcjJ1Ww9eY2v9lxkz/lYbG0U3RpWY0Cr+2ji\nWTznTrd0+/btY/jw4SxcuJCmTZsyZ84coyNZvAIVvlKqM/ApYAMs0lpPveV1T+AroOL1ZcZrrTcU\nclYhLE58ahbfHrjMt/svE5GQTtXyDrzWsS5PNPOgWkXrPPWvqMXHx/O///2P+fPnU61aNeLj442O\nVGzkW/hKKRtgDtARCAf+UEqt1VqfuGmxCcD3WuvPlVL1gQ2AVxHkFcJwf55SuWzvRTaERJKVnUvr\n2s68+agvnepXobRN8brtXXGyYsUKRo0aRUxMDKNHj+bdd9/FycnJ6FjFRkH28FsA57TWoQBKqeVA\nD+DmwtdA+etfVwCuFGZIISxBVnYuG0OusnBXKCERSTg5lObJAA/6t/SiXlUpHXM4deoUXl5ebNq0\niSZNmhgdp9jJ9wYoSqkngM5a6xeuP+4PBGqtR9y0jDuwBagElAMe1loH/9fnyg1QRHGRlGFiVXA4\nX+y+QFhcOrVcyzGwtRe9mnpQTiYuK1IZGRlMmzaNpk2b0r17d0wmE6VKlbKKic7+jSXcAKUvsERr\n/aFSqhWwTCnlr7XOvXkhpdQQYAiAp6dnIa1aiKJxNDyBpXsvse7IFTKzc2niWZGJ3fzo4OMmN/g2\ng61btzJs2DDOnj3LmDFj6N69O7a2cs3CvShI4UcANW567HH9uZsNBjoDaK33KqUcABcg6uaFtNYL\ngAWQt4d/l5mFKDK5uZptp6OYt+M8f1yMp6ydDY83qc4zgZ409KhodDyrcO3aNV577TW+/fZbvL29\n2bJlCx07djQ6VolQkML/A6ijlKpJXtE/DTxzyzKXgYeAJUopX8ABiC7MoEIUpdxczcaQSGb9epbT\n15Jxr+DAW93q82SAh1wJa2a//PILP/zwA2+//TZvvPEGDg4ORkcqMfItfK11tlJqBLCZvFMuv9Ba\nH1dKvQcEaa3XAmOAhUqpV8k7gPu8Nuru6ELcgeycXDaERDLnt3OcvpZMLddyfPp0Y7o2cMdWzrYx\nmyNHjnD27FmeeOIJnn32We6//35q1qxpdKwSJ9+DtkVFDtoKI2Vm57AyKJyFu0K5FJuGt5sjIx70\npnujatjI+LzZpKSkMHHiRD799FO8vLw4deoUpUvLgfD/YgkHbYUoFjJMOXwfFMbn289zNTGDxjUq\nMq6zD4/4VZWiN7PVq1czcuRIwsPDGTJkCFOmTJGyL2KydYVViE3JZOGuCyz/4zIJaSaa3VeJab0b\nyg1GDHLs2DEef/xxGjRowIoVK2jdurXRkayCFL4o0cLi0lj8+wVWBoWRZsqhs19VBrTyomUtmXve\n3EwmE7t27aJDhw40aNCA9evX07FjRznV0oyk8EWJFB6fxuzfzrHqYDgAjzZwZ0QHb7zd5IpYI+zZ\ns4ehQ4dy/PhxTp8+jbe3N127djU6ltWRwhclyqXYVD7depY1R65gU0rxdHNPhj/oTdUKcmqfEeLi\n4hg/fjwLFy6kRo0a/Pjjj3h7exsdy2pJ4YsSITHNxNzt5/hy90WUgudbezG4TU2ZsdJAGRkZNG7c\nmCtXrjBmzBjeeecdHB3lxi9GksIXxVpyholv9l9mzrZzpGZm07NxdcZ18aFKedmjN0p4eDgeHh44\nODgwadIkGjduTKNGjYyOJZDCF8VUZnYOy/Ze4vPt54lNzaJtXVfe6OKDr3v5/N8sikR6ejpTpkxh\n2rRp/PDDD3Tv3p3nnnvO6FjiJlL4oljRWrP5eCQfbDjF5bg0Wtd2Zuwj9eSuUgbbsmULw4YN4/z5\n8/Tr148WLVoYHUnchhS+KDYOXY5nysZTHLgQRx03R5YOakHbuq5Gx7J6I0eOZPbs2dSpU4etW7fy\n0EMPGR1J/AspfGHxwuPTeP/nk2w6HolzOTsmP+7PUwE15M5SBsrJyQHAxsaGli1b4uLiwrhx42Si\nMwsnhS8sVnZOLgt2hTLr17MAjH64Di88UAtHuemIoQ4ePMjQoUPp378/I0eO5NlnnzU6kigg+Zcj\nLI7Wmg3HIvnol9Ocj07lEb8qvN3dj+pyiqWhkpOTefvtt5k1axaurq64u7sbHUncISl8YVFCo1N4\nd90JdpyJxtvNkQX9m9HJr6rRsazeli1bGDRoEFeuXGHo0KF88MEHVKwoN4QpbqTwhUVITDPx4S+n\n+Wb/ZexsSvF2t/o819pLZrC0EHZ2dri5ubFq1SoCAwONjiPukhS+MFR2Ti4rgsL4cMsZEtKyeDbw\nPkY9VAdXJ3ujo1k1k8nERx99RFJSEpMnT6Z9+/YEBQVRqpQcKC/OpPCFYX47dY0pG05xNiqFFl6V\neecxP+pXkwunjPb777/fmOisT58+5ObmUqpUKSn7EkAKX5hdWFwak34+wZYT16jlWo55/ZryiF9V\nma7YYLGxsYwbN47Fixfj6enJunXr6Natm9GxRCGSwhdmk52Ty9K9l5i26RSllOL/OtfjhTa1sCst\ne46WIDY2luXLl/N///d/vP3225QrV87oSKKQSeELs9hzPoZ31h7nzLUUHqznyuTHG8hMlhbg5MmT\nfP/990ycOJG6dety+fJlKleubHQsUUSk8EWRikrOYMqGU/x0KILqFcvw+bNN6ewvwzdGS0tLY/Lk\nycyYMQNHR0cGDx6Mh4eHlH0JJ4UvikROrubL3Rf4dOtZMrNzGfGgNyM6eONga2N0NKu3adMmhg0b\nxoULF3juueeYMWMGrq4yJ5E1kMIXhe50ZDJv/HiUg5cTaFfXlYnd61PLVW58YQlSUlLo378/zs7O\nbNu2jfbt2xsdSZiRFL4oNLm5ms93nOfjX87g6FCaj59qRM/G1WX4xmA5OTl899139O3bF0dHR7Zu\n3YqPjw/29nKtg7WRwheFIjQ6hfE/HuPAhTi6NXTnvR7+VC5nZ3QsqxccHMxLL71EcHAwZcqUoXfv\n3nL3KSsmhS/uSXZOLt8euMyUDacobaOY2qsBTzWvIXv1BktMTOStt95izpw5uLm5sXz5cnr16mV0\nLGEwKXxx146GJzBu1TFOXk3igTouzOzTSO4layF69+7Nb7/9xvDhw3n//fepUKGC0ZGEBZDCF3cs\nJTObj385w1d7LuLiaM/sZ5rwaAN32as3WGhoKK6urjg5OTF58mRKlSpF8+bNjY4lLIhc4ijuyI4z\n0Tz04XYW/36BPgEebB7dlm4Nq0nZGygrK4sPPvgAPz8/3n//fQACAwOl7MU/yB6+KJC0rGymbTzF\nV3svUcfNkc/7NaOp3DjccDt37mTo0KGcPHmSJ554glGjRhkdSVgwKXyRrwMX4hi/6iihMakMvN+L\ncZ195AIqC/Dxxx/z2muv4eXlxfr16+natavRkYSFk8IX/yonV/Pp1jPM3nYO9wpl+PaFQFp7uxgd\ny6rl5uaSmpqKk5MTjz76KNHR0UyYMIGyZcsaHU0UA0prbciKAwICdFBQkCHrFvkLi0tj7A9H2Bca\nR++mHrzXw49ycvNwQx0/fpyhQ4feuPOUsE5KqWCtdcDdvLdAB22VUp2VUqeVUueUUuP/ZZknlVIn\nlFLHlVLf3k0YYTytNT8eDKfLp7sIiUhi+hMN+fDJRlL2BkpLS+ONN96gcePGnDx5km7dumHUjpoo\n3vL9V6yUsgHmAB2BcOAPpdRarfWJm5apA7wB3K+1jldKuRVVYFF0opIyeHN1CL+cuEYLr8rM7NMI\nT2cZKjDSoUOH6NWrFxcvXmTgwIFMnz4dFxcZVhN3pyC7bS2Ac1rrUACl1HKgB3DipmVeBOZoreMB\ntNZRhR1UFK2tJ67x6veHyczOZXwXH15oU5PSNnLWrlG01iil8PT0xNPTk6+++oq2bdsaHUsUcwUp\n/OpA2E2Pw4Fbb1tfF0AptRuwAd7RWm8qlISiSOXkamZuOc28Hefxr1aBT59uLDNbGig7O5vZs2ez\ndu1afvnlF5ydndmxY4fRsUQJUVi7cKWBOkB7oC+wUClV8daFlFJDlFJBSqmg6OjoQlq1uFsxKZkM\nXPIHn28/z5PNavD9S62k7A104MABWrRowauvvoqDgwNJSUlGRxIlTEEKPwKocdNjj+vP3SwcWKu1\nNmmtLwBnyPsB8Dda6wVa6wCtdYDccMFYBy/H0/2z39kXGsv7Pf2Z9kRDytjJufVGSElJYfjw4bRs\n2ZJr166xcuVK1q9fT6VKcmGbKFwFKfw/gDpKqZpKKTvgaWDtLcusJm/vHqWUC3lDPKGFmFMUktTM\nbCavP0GfeXspbaNYNbQ1/VreZ3Qsq2Zra8v27dsZOXLkjStmZaoKURTyHcPXWmcrpUYAm8kbn/9C\na31cKfUeEKS1Xnv9tU5KqRNADjBWax1blMHFnTt5NYnh3xwkNCaVp5vX4I0uvlQoa2t0LKt07tw5\n3nvvPebMmYOTkxPBwcE4OMhMo6JoyYVXVkBrzfdBYby95jjly9jy6dONaV1bTu0zQmZmJtOnT2fy\n5MnY2dmxfv16HnjgAaNjiWLkXi68kqtpSris7FwmrD7G90HhBNaszOxnmuLqJLe2M8K2bdt4+eWX\nOX36NE899RQfffQR1apVMzqWsCJS+CVYWFwao5Yf4tDlBEZ28ObVh+tSqpSMDRtBa83kyZMxmUxs\n2rSJRx55xOhIwgpJ4ZdAWmt+OhTBW6tDUEox55mmPNrQ3ehYVic3N5fFixfTuXNnatSowbJly6hY\nsSJlypQxOpqwUnIpZQmTmZ3D6yuP8tr3R/CrXoGNrzwgZW+Ao0eP0qZNG4YMGcKiRYsAcHd3l7IX\nhpI9/BLkSkI6I749yMHLCYx6qA6jOnjL9AhmlpKSwrvvvsvHH39MpUqVWLJkCQMGDDA6lhCAFH6J\ncTgsgSFLg0jNzJYhHAO98847fPjhh7zwwgtMnToVZ2dnoyMJcYMUfjGntebbA5d5Z+1x3Jwc+HHY\n/dSr6mR0LKsSFhZGamoqPj4+jB8/np49e9KmTRujYwnxD/L7fjGWYcrhjR+P8eZPIbSs5czPI9tI\n2ZtRdnY2H330Eb6+vrz00ksAuLi4SNkLiyV7+MXUiStJjPzuIOejUxnWvjavd6onp1ya0b59+xg6\ndChHjhzh0UcfZfbs2UZHEiJfUvjF0Jbjkby64jCODqVZMrA57evJ/WbMaf369XTv3p1q1arx448/\n0rNnT5n7RhQLMqRTzCz+/QIvfR1MbTdH1gxvI2VvJlprIiLyJol9+OGHee+99zh58iSPP/64lL0o\nNqTwi4kMUw5vrQ5h0s8n6OhbhRVDWlG1gky2ZQ5nzpyhY8eOtGrVipSUFOzt7ZkwYQJOTnK8RBQv\nMqRTDMSlZtFv0X5OXE3ixQdqMr6LLzYyXl/kMjIymDp1KlOmTKFMmTI3/itEcSWFb+FORSYxdFkw\nVxIzWDgggI71qxgdySpERkbStm1bzp49S9++ffnoo4+oWrWq0bGEuCdS+BZsy/FIXlmed3D2uxcD\naXZfZaMjlXgmkwlbW1uqVKlC27ZtmTNnDh07djQ6lhCFQsbwLdTSvRd56etg6lZxZP3INlL2RSw3\nN5d58+ZRu3ZtwsPDUUqxaNEiKXtRokjhW5jcXM1Hv5zh7TXHecinCsuHtMKtvBycLUpHjhyhdevW\nvPzyy9SpUweTyWR0JCGKhBS+Bckw5TBkWTCzfj1Lr6bV+bxfU7mxeBHSWvP666/TrFkzQkNDWbZs\nGVu3bqVmzZpGRxOiSMgYvoXIzM5h8Fd/sOd8LBO71+f51l5yfncRU0oRHx/P4MGDmTp1KpUqVTI6\nkhBFSvbwLYApJ5eXlgWz+1ws03s3ZOD9NaXsi8ilS5fo2bMnBw8eBGDhwoXMnz9fyl5YBSl8g2Vm\n5zDy20NsPx3N+z396RNQw+hIJZLJZGL69OnUr1+fX375hdOnTwNQqpT8ExDWQ4Z0DJSYZmLIsiD2\nX4jjrW716dfyPqMjlUh79uzhpZdeIiQkhB49ejBr1iw8PT2NjiWE2UnhGyQhLYtBS/4gJCKJT59u\nTI/G1Y2OVGJt3bqVxMREVq9eTY8ePYyOI4RhlNbakBUHBATooKAgQ9ZttPjULJ5esI/QmBQ+69uE\nzv5yd6rCpLVm2bJluLq60qVLFzIzMzGZTDg6OhodTYh7ppQK1loH3M17ZQDTzBLTTfT/Yj8XYlL5\n8vkWUvaF7NSpU3To0IHnnnuOL7/8EgB7e3speyGQwjerqOQM+i3az+nIZOb1b0qbOi5GRyox0tPT\neeutt2jYsCGHDx9m/vz5LF++3OhYQlgUGcM3k8uxafRbvJ/o5Ezm9WtGBx+ZBK0wrVu3jvfff59+\n/foxc+ZMqlSR7SvEraTwzeBSbCp9F+wjNSuHb18MpImnnPNdGCIjIzl8+DCdO3emT58+eHl50aJF\nC6NjCWGxZEiniF1NTKff4v2km6TsC0tOTg5z586lXr169O/fn/T0dJRSUvZC5EMKvwhFJWXQf/EB\n4lNNfDmwBX7VKhgdqdg7ePAgrVq1Yvjw4bRo0YI9e/bITUmEKCAZ0ikiF2NSee7LA0QnZ7L4ueY0\nrlHR6EjF3oULF2jRogUuLi58++23PP300zIFhRB3QAq/CJy9lsyzi/Zjysnl6xcCaSrDOHdNa82x\nY8do2LAhNWvW5Msvv6R79+5UrCg/QIW4UwUa0lFKdVZKnVZKnVNKjf+P5XorpbRS6q4uCigJzken\n0HfhPjSw4qVWUvb34MKFC3Tr1o0mTZpw9OhRAPr37y9lL8RdyrfwlVI2wBygC1Af6KuUqn+b5ZyA\nV4D9hR2yuLgUm0r/RfvRGpYPaUndKk5GRyqWsrKymDp1Kn5+fuzYsYOZM2dSv/4/vuWEEHeoIEM6\nLYBzWutQAKXUcqAHcOKW5SYB04CxhZqwmAiPT+OZhftJM+Xw9eBAarvKlZ13Iycnh9atWxMcHEyv\nXr345JNPqFFDZhAVojAUZEgPjOOGAAAYVUlEQVSnOhB20+Pw68/doJRqCtTQWq8vxGzFxtXEdJ5d\ntJ+kDBNfDw7Ev7qcjXOnkpKSALCxsWHQoEGsW7eOVatWSdkLUYju+bRMpVQp4CNgTAGWHaKUClJK\nBUVHR9/rqi1CVFIGzyzcT2xKFl8NaiFlf4e01ixZsoRatWqxZs0aAIYNG0a3bt0MTiZEyVOQwo8A\nbt7N8rj+3J+cAH9gu1LqItASWHu7A7da6wVa6wCtdYCrq+vdp7YQqZnZvLg0iMjEDJYMbC4HaO/Q\niRMnaN++PQMHDsTHx4fatWsbHUmIEq0ghf8HUEcpVVMpZQc8Daz980WtdaLW2kVr7aW19gL2AY9p\nrUv03MeZ2TmM+PYgxyISmdW3CQFelY2OVKxMnz6dRo0aERISwqJFi9i5cyf+/v5GxxKiRMu38LXW\n2cAIYDNwEvhea31cKfWeUuqxog5oiXJyNcO/OcS209FM6ulPx/oyUVdB/Xn/hapVq/Lss89y6tQp\nBg8eLLcaFMIM5AYod0hrzfhVx1gRFMbE7vUZeH9NoyMVC1euXOGVV17hgQceYNSoUUbHEaLYkhug\nmInWmqkbT7EiKIwRD3pL2RdATk4On332GT4+Pvz888/k5OQYHUkIqyVTK9yBudvPM39nKP1aejKm\nU12j41i8w4cP88ILLxAcHEynTp2YO3euHJgVwkBS+AW0+XgkM7ecpkfjarz3mL9M2lUAiYmJXLly\nhRUrVtCnTx/ZZkIYTAq/APaej2X08sM09KjI1F4NKVVKiut2tNasXLmSs2fP8uabb9KuXTtCQ0Nx\ncHAwOpoQAhnDz9fR8ARe+OoPqlV0YOGAZpSxszE6kkU6f/48Xbt25amnnmLNmjWYTCYAKXshLIgU\n/n84F5XCwC//oGJZO75+IRA3JymvW2VmZjJ58mT8/f3ZvXs3n376KXv27MHW1tboaEKIW8iQzr+I\nS82i/+L9KAVLB7fAvYLcVel2wsLCmDRpEt27d+eTTz6hevXq+b9JCGEI2cO/jczsHIZ+HUxsShZL\nBraQmS9vER0dzezZswHw9vbmxIkTrFy5UspeCAsnhX+L3FzNuB+OcuBCHNOfaCiTod0kNzeXxYsX\n4+Pjw2uvvcbp06cBqFWrlsHJhBAFIYV/i6mbTrH68BVe71SXnk1kj/VPISEhtGvXjhdeeAE/Pz8O\nHz5MvXr1jI4lhLgDMoZ/k6/3XWLBzlAGtLqP4Q96Gx3HYmRlZdGpUyeysrL44osveP755+WceiGK\nISn86/aFxvLO2uO0r+fK293qS6EBv/32G+3atcPOzo7vv/8eHx8fXFxcjI4lhLhLMqQDRCSkM/yb\ng3g6l+XTp5tQ2sa6N0t4eDi9e/fmoYceYunSpQC0adNGyl6IYs66m43rZ+QsCyYrO5cF/QOoUMZ6\nzx/Pzs7mk08+wdfXl40bNzJlyhSeffZZo2MJIQqJ1Q/pzNh0mmMRiczv3wxvN+s+/bJ///4sX76c\nLl26MGfOHGrWlNlAhShJrLrwNx67yqLfL/BsoCeP+FU1Oo4hEhISKF26NI6OjgwfPpzevXvTu3dv\nOYYhRAlktUM6R8ISGL3iMI1rVOTt7vWNjmN2WmuWL1+Or68vb731FpA3Tv/EE09I2QtRQlll4Ucn\nZ/LSsmBcHO1Z/FwA9qWta0K0c+fO8cgjj9C3b188PDzo16+f0ZGEEGZgdYWfk6sZveIQCelZzO/f\nDGdHe6MjmdW3336Lv78/+/fvZ/bs2ezbt49mzZoZHUsIYQZWN4a/+PdQdp+LZUqvBlY1bYLJZMLW\n1paAgACeeOIJpk+fTrVq1YyOJYQwI6vaww+6GMeMzafpVL8KTzevYXQcs4iKiqJ///489dRTANSt\nW5evv/5ayl4IK2Q1hR+fmsWIbw/hXqEMM55oVOIPTObm5rJgwQLq1avHihUr8PPzkxuIC2HlrGJI\nJzdXM27VUWJTM/nx5fupULZkX1wVGhpKv3792Lt3L+3bt+fzzz/Hx8fH6FhCCINZReEv2BXKlhPX\nmPCoLw08Sv64fYUKFUhISOCrr76if//+Jf63GSFEwZT4IZ1Dl+OZufk0nf2qMrhNyb1ydO3atfTq\n1YucnBycnZ0JCQlhwIABUvZCiBtKdOGnZ+UwavkhqlZwYGrvBiWy/C5fvkzPnj3p0aMHZ86c4erV\nqwCUKlWi/9cKIe5CiW6F99efIDw+nRlPNKJiWTuj4xSq7OxsZs6cia+vL1u2bGHatGkcOnQIDw8P\no6MJISxUiR3D33U2mm/2X+aFNjVpVdvZ6DiFLicnh0WLFtGhQwc+++wzvLy8jI4khLBwJXIPPy41\ni9dXHqGWSzlef6Tk3IYvPj6ecePGkZycjL29Pbt372bt2rVS9kKIAimRhT/p5xPEpmQxq28THGyL\n/zw5Wmu++eYbfHx8+PDDD9m2bRsAzs7OJfK4hBCiaJS4wl9zOIKfDkUw7EHvEjF1wpkzZ+jYsSP9\n+vXDy8uLoKAgHnvsMaNjCSGKoRI1hh+fmsW7607QuEZFRnUoGTchHz16NEFBQcydO5chQ4ZgY1P8\nf2MRQhijxBS+1pr//XSM5AwTHzzeoFjfl/aXX37Bx8eHGjVq8Pnnn2Nvb0/VqtZ5gxYhROEpUCsq\npTorpU4rpc4ppcbf5vXXlFInlFJHlVK/KqXuK/yo/23tkStsDIlk9MN1qV+tvLlXXygiIyN55pln\n6NSpE9OmTQPgvvvuk7IXQhSKfAtfKWUDzAG6APWBvkqpW28RdQgI0Fo3BH4Aphd20P8Sd30op1GN\nigxtV9ucqy4Uubm5zJs3Dx8fH1atWsXEiROZOXOm0bGEECVMQfbwWwDntNahWussYDnQ4+YFtNbb\ntNZp1x/uA8x69c/760+QlG5ieu+G2JQqfmetTJkyhZdffplmzZpx9OhR3nnnHRwcHIyOJYQoYQoy\nhl8dCLvpcTgQ+B/LDwY23u4FpdQQYAiAp6dnASP+t99OXePHgxEMa1+belWdCuUzzSE5OZmYmBhq\n1qzJ0KFDqVmzJn379pXTLIUQRaZQj2wqpfoBAcCM272utV6gtQ7QWge4urre8/oyTDlMXHuc2q7l\neOXhOvf8eeagteann36ifv36PPXUU2itcXZ25plnnpGyF0IUqYIUfgRw8+2hPK4/9zdKqYeBN4HH\ntNaZhRPvv83dfp6wuHQm9fAvFjciv3TpEo899hi9evWicuXKzJo1S0peCGE2BRnS+QOoo5SqSV7R\nPw08c/MCSqkmwHygs9Y6qtBT3kbQxThm/3aWXk2q09rbxRyrvCd79+7l4YcfBmDmzJm88sorlC5d\nYs6KFUIUA/nu4Wuts4ERwGbgJPC91vq4Uuo9pdSfl3zOAByBlUqpw0qptUWWGMjOyWXC6hDcK5Th\n3R5+Rbmqe5aUlARA06ZNGTRoECdPnmTMmDFS9kIIsytQ62itNwAbbnnu7Zu+friQc/2nL3Zf4FRk\nMvP6NcXJwTJvVxgbG8v48ePZsmULx48fx9HRkc8++8zoWEIIK1bsLkeNTMzgk61necjHjUf8LO+C\nJK01S5cuxcfHhy+//JKnnnpKxumFEBah2I0rfPzLGbKyc5nY3c/iijQxMZGePXuyfft2WrVqxbx5\n82jYsKHRsYQQAihmhX8xJpWVwWEMaOWFp3NZo+PcoLVGKUX58uVxcXFhwYIFDB48WG4zKISwKMWq\nkd5ffwL70jYMe9Bypk/YvHkzTZs2JTw8HKUUK1eu5MUXX5SyF0JYnGLTSocux7P1ZBQjOnjj5mT8\ntANXr17l6aefpnPnzqSlpREVZZazUYUQ4q4Vi8LPzdW8s+4ELo72DGhl9ok4/2HOnDn4+PiwevVq\n3n33XY4ePUrTpk2NjiWEEP+pWIzhrz1yhSNhCczs08giTsMMDg4mMDCQOXPmUKdO8ZjSQQghLH4P\nPzUzmykbT9KgegV6NaluSIakpCRGjx5NcHAwAHPnzmXz5s1S9kKIYsXiC3/hrlCuJWXyzmN+lDLz\n1Mdaa3744Qd8fX2ZNWsWO3bsAMDBwcHiTgkVQoj8WHThJ6Rl8cXvF3jYtwrN7qtk1nVfuHCBbt26\n0adPH9zc3Ni7dy+vvfaaWTMIIURhsujCn78zlOTMbMZ0qmv2dX/zzTfs3LmTjz/+mD/++IPAwP+6\nBYAQQlg+iz1oey0pgy9+v0D3htXwdTfPPWp37dpFZmYmDz/8MGPHjuX555/Hw8OsN+8SQogiY7F7\n+J9sPUOu1rzWsej37mNiYhg0aBBt27blvffeA8De3l7KXghRoljkHn5odArfB4XTL9ATL5dyRbYe\nrTVLlixh7NixJCYmMm7cON56660iW58QxZHJZCI8PJyMjAyjo1gVBwcHPDw8sLUtvFPRLbLwF+wM\nxaaUYuRDRXva44YNGxg0aBD3338/8+bNw9/fv0jXJ0RxFB4ejpOTE15eXnJ2mplorYmNjSU8PJya\nNWsW2uda3JBOVHIGPx6KoHdTD1wc7Qv989PS0ti9ezcAXbt2Zc2aNezcuVPKXoh/kZGRgbOzs5S9\nGSmlcHZ2LvTfqiyu8JftvYQpJ5chbWsV+mdv3LgRf39/unTpQkJCAkopHnvsMZnoTIh8SNmbX1Fs\nc4tquqQME8v2XeJh3yrULMSx+4iICPr06UPXrl2xt7dn3bp1VKxYsdA+XwhhXYKDg2nQoAHe3t6M\nGjUKrfU/lpkxYwaNGzemcePG+Pv7Y2NjQ1xcHACDBg3Czc3N7CMLFlX43+y7TEKaiVEdCm/sPioq\nivr16/Pzzz/z/vvvc+TIEdq1a1dony+EMF5OTo5Z1/fyyy+zcOFCzp49y9mzZ9m0adM/lhk7diyH\nDx/m8OHDTJkyhXbt2lG5cmUAnn/++du+p6hZTOFnZuew+PcL3O/tTAOPCvf8eREREQC4ubkxadIk\nQkJCePPNN7Gzs7vnzxZCmE/Pnj1p1qwZfn5+LFiw4Mbzjo6OjBkzhkaNGrF3716Cg4Np164dzZo1\n45FHHuHq1asALFy4kObNm9OoUSN69+5NWlraPeW5evUqSUlJtGzZEqUUAwYMYPXq1f/5nu+++46+\nffveeNy2bdsb5W9OFnOWztrDV4hJyeTjdo3u6XMSExOZMGEC8+fPZ9++fTRt2pRRo0YVUkohrNu7\n645z4kpSoX5m/Wrlmdjd719f/+KLL6hcuTLp6ek0b96c3r174+zsTGpqKoGBgXz44YeYTCbatWvH\nmjVrcHV1ZcWKFbz55pt88cUX9OrVixdffBGACRMmsHjxYkaOHPm3dWzbto1XX331H+suW7Yse/bs\n+dtzERERf7tGx8PD48YO5u2kpaWxadMmZs+eXaDtUZQsovC11izYGYpPVSfaeLvc9WesXLmS0aNH\nExkZyYgRI6hd23LujCWEuDuzZs3ip59+AiAsLIyzZ8/i7OyMjY0NvXv3BuD06dOEhITQsWNHIG+I\nx93dHYCQkBAmTJhAQkICKSkpPPLII/9Yx4MPPsjhw4eLJP+6deu4//77Ddmjv5VFFP72M9GcjUrh\nwz6N7urItNaaXr16sXr1apo2bcratWsJCAgogqRCWLf/2hMvCtu3b2fr1q3s3buXsmXL0r59+xun\nKjo4OGBjYwPkdYCfnx979+79x2c8//zzrF69mkaNGrFkyRK2b9/+j2XuZA+/evXqhIeH33gcHh5O\n9er/PnX78uXL/zacYySLKPzv9l/GxdGO7o2q3dH7TCYTtra2KKVo06YNHTp0YNiwYTe+CYQQxVti\nYiKVKlWibNmynDp1in379t12uXr16hEdHc3evXtp1aoVJpOJM2fO4OfnR3JyMu7u7phMJr755pvb\nlvOd7OG7u7tTvnx59u3bR2BgIEuXLv3HENHN+Xfs2MHXX39d8L90ETL8oG1YXBpbT16jdzMP7EoX\nPM727dtp2LAha9asAWDMmDGMHDlSyl6IEqRz585kZ2fj6+vL+PHjadmy5W2Xs7Oz44cffmDcuHE0\natSIxo0b39gznzRpEoGBgdx///34+PgUSq65c+fywgsv4O3tTe3atenSpQsA8+bNY968eTeW++mn\nn+jUqRPlyv39NPO+ffvSqlUrTp8+jYeHB4sXLy6UXPlRtzt/1BwCAgJ0UFAQ0zedYt6O8/w+rgPV\nKpbJ933R0dG8/vrrLF26lJo1a7Jo0SI6dOhghsRCWKeTJ0/i6+trdAyrdLttr5QK1lrf1Zi1oXv4\n2Tm5rAwOp4OPW4HK/rvvvqNevXp89913/O9//yMkJETKXgghCsjQMfztp6OJTs6kT0CNAi2fnZ2N\nv78/8+bNo379+kWcTgghShZD9/BXH46gcjk7Ovi43fb11NRUxo8fz9y5cwHo168fO3bskLIXQoi7\nYFjh52rN1pPX6OJfFVubf8b4+eef8fPzY9q0aZw5cwbIm0xIJnESwvyMOtZnzYpimxtW+Enp2WSY\ncunW8O+nYoaHh9OrVy+6d+9OuXLl2LlzJ5988olBKYUQDg4OxMbGSumb0Z/z4Ts4OBTq5xo2hp+Q\nloVv5TIE1vz71WehoaFs3ryZKVOm8Nprr8ncN0IYzMPDg/DwcKKjo42OYlX+vONVYSpQ4SulOgOf\nAjbAIq311FtetweWAs2AWOAprfXF//rMlMxsHvKpQqlSigMHDrB3715eeeUV2rZty+XLl3F2dr6b\nv48QopDZ2toW6l2XhHHyHdJRStkAc4AuQH2gr1Lq1qOmg4F4rbU38DEwLb/P1YC/S2mGDRtGy5Yt\n+eijj0hNTQWQshdCiCJQkDH8FsA5rXWo1joLWA70uGWZHsBX17/+AXhI5XN0NTc9meGPt2P+/PmM\nGjWKY8eO/eNqNCGEEIWnIEM61YGwmx6HA4H/tozWOlsplQg4AzH/9qHZiVF4BjRj48YNNG3a9M5S\nCyGEuGNmPWirlBoCDLn+MDMoKCikWbNm5oxgqVz4jx+OVka2xV9kW/xFtsVf6t3tGwtS+BHAzZfC\nelx/7nbLhCulSgMVyDt4+zda6wXAAgClVNDdzgdR0si2+Itsi7/ItviLbIu/KKWC7va9BRnD/wOo\no5SqqZSyA54G1t6yzFrguetfPwH8puWkXSGEsCj57uFfH5MfAWwm77TML7TWx5VS7wFBWuu1wGJg\nmVLqHBBH3g8FIYQQFqRAY/ha6w3Ahluee/umrzOAPne47gX5L2I1ZFv8RbbFX2Rb/EW2xV/uelsY\nNh++EEII8zL8jldCCCHMo8gLXynVWSl1Wil1Tik1/jav2yulVlx/fb9SyquoMxmlANviNaXUCaXU\nUaXUr0qp+4zIaQ75bYubluutlNJKqRJ7hkZBtoVS6snr3xvHlVLfmjujuRTg34inUmqbUurQ9X8n\nXY3IWdSUUl8opaKUUiH/8rpSSs26vp2OKqUKdjGT1rrI/pB3kPc8UAuwA44A9W9ZZhgw7/rXTwMr\nijKTUX8KuC0eBMpe//pla94W15dzAnYC+4AAo3Mb+H1RBzgEVLr+2M3o3AZuiwXAy9e/rg9cNDp3\nEW2LtkBTIORfXu8KbAQU0BLYX5DPLeo9/CKZlqGYyndbaK23aa3Trj/cR941DyVRQb4vACaRNy9T\nhjnDmVlBtsWLwBytdTyA1jrKzBnNpSDbQgPlr39dAbhixnxmo7XeSd4Zj/+mB7BU59kHVFRKuef3\nuUVd+LeblqH6vy2jtc4G/pyWoaQpyLa42WDyfoKXRPlui+u/otbQWq83ZzADFOT7oi5QVym1Wym1\n7/rstSVRQbbFO0A/pVQ4eWcOjjRPNItzp30CGHxPW3F7Sql+QADQzugsRlBKlQI+Ap43OIqlKE3e\nsE578n7r26mUaqC1TjA0lTH6Aku01h8qpVqRd/2Pv9Y61+hgxUFR7+HfybQM/Ne0DCVAQbYFSqmH\ngTeBx7TWmWbKZm75bQsnwB/YrpS6SN4Y5doSeuC2IN8X4cBarbVJa30BOEPeD4CSpiDbYjDwPYDW\nei/gQN48O9amQH1yq6IufJmW4S/5bgulVBNgPnllX1LHaSGfbaG1TtRau2itvbTWXuQdz3hMa33X\nc4hYsIL8G1lN3t49SikX8oZ4Qs0Z0kwKsi0uAw8BKKV8ySt8a7wV11pgwPWzdVoCiVrrq/m9qUiH\ndLRMy3BDAbfFDMARWHn9uPVlrfVjhoUuIgXcFlahgNtiM9BJKXUCyAHGaq1L3G/BBdwWY4CFSqlX\nyTuA+3xJ3EFUSn1H3g95l+vHKyYCtgBa63nkHb/oCpwD0oCBBfrcErithBBC3IZcaSuEEFZCCl8I\nIayEFL4QQlgJKXwhhLASUvhCCGElpPCFEMJKSOELIYSVkMIXQggr8f8WTsFPqBB2MAAAAABJRU5E\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1feeecf668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Create ROC curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pred_probas = lr.predict_proba(gensim_vectors_test)[:,1]\n",
    "\n",
    "fpr,tpr,_ = roc_curve(y_test, pred_probas)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "plt.plot(fpr,tpr,label='area = %.2f' %roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC # \"Support vector classifier\"\n",
    "# clf = SVC(kernel='rbf', C=1E6)\n",
    "# clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1280000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gensim_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-3b3e177be052>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# fix random seed for reproducibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=300, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(5, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model.fit(X, Y, epochs=5, batch_size=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = model.evaluate(X, Y)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-5ec36f42f1a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0malg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_samples_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Compute the accuracy score for all of the cross validation folds; this is much simpler than what we did before\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mkf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=100, min_samples_split = 20000, min_samples_leaf=10000)\n",
    "# Compute the accuracy score for all of the cross validation folds; this is much simpler than what we did before\n",
    "kf = cross_validation.KFold(y_train.shape[0], n_folds=3, random_state=1)\n",
    "scores = cross_validation.cross_val_score(alg, train_vectors, y_train, cv=kf)\n",
    "\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print(scores.mean())\n",
    "\n",
    "#fitting the model\n",
    "alg.fit(train_vectors, train_label)\n",
    "print (alg.score(train_vectors, train_label))\n",
    "# Predict Output\n",
    "y_pred= alg.predict(test_vectors)\n",
    "\n",
    "#test error\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "C = confusion_matrix(test_vectors,y_pred)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove punctuations\n",
    "def cleanText(corpus):\n",
    "    punctuation = \"\"\".,?!:;(){}[]\"\"\"\n",
    "    corpus = [z.lower().replace('\\n','') for z in corpus]\n",
    "    corpus = [z.replace('<br />', ' ') for z in corpus]\n",
    "\n",
    "    #treat punctuation as individual words\n",
    "    for c in punctuation:\n",
    "        corpus = [z.replace(c, ' %s '%c) for z in corpus]\n",
    "    corpus = [z.split() for z in corpus]\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.70, min_df=2, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(feedback_data[\"feedback_cleaned\"])\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
