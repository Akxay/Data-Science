{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Building a large scale Movie Recommendation Engine using pyspark **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import datetime\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import itertools\n",
    "from math import sqrt\n",
    "from operator import add\n",
    "from os.path import join, isfile, dirname\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "from pyspark.sql.types import TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f9621678550>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conf = SparkConf().setAppName(\"MovieRecommendationEngine\")\n",
    "# sc = SparkContext(conf=conf)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size is  100004\n"
     ]
    }
   ],
   "source": [
    "# load Ratings file into df\n",
    "datasets_path=os.getcwd() + \"/RE_data\"\n",
    "ratings_file = os.path.join(datasets_path, 'ratings.csv')\n",
    "ratings_raw_data = sc.textFile(\"file:///\" + ratings_file)\n",
    "ratings_raw_data_header = ratings_raw_data.take(1)[0]\n",
    "ratings_data = ratings_raw_data.filter(lambda line: line != ratings_raw_data_header)\\\n",
    "    .map(lambda line: line.split(\",\")).map(lambda tokens: (int(tokens[0]),int(tokens[1]),int(float(tokens[2])))).cache()\n",
    "\n",
    "print \"train size is \", ratings_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, u'Toy Story (1995)'),\n",
       " (2, u'Jumanji (1995)'),\n",
       " (3, u'Grumpier Old Men (1995)')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load courses file into df\n",
    "movies_file = os.path.join(datasets_path, 'movies.csv')\n",
    "movies_raw_data = sc.textFile(\"file:///\" + movies_file)\n",
    "movies_raw_data_header = movies_raw_data.take(1)[0]\n",
    "\n",
    "movies_data = movies_raw_data.filter(lambda line: line!=movies_raw_data_header)\\\n",
    "    .map(lambda line: line.split(\",\")).map(lambda tokens: (int(tokens[0]),tokens[1])).cache()\n",
    "movies_data.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train, validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 59824, validation: 20190, test: 19990\n"
     ]
    }
   ],
   "source": [
    "rddTraining, rddValidating, rddTesting = ratings_data.randomSplit([6,2,2], seed=1001)\n",
    "\n",
    "#Add user ratings in the training model\n",
    "nbValidating = rddValidating.count()\n",
    "nbTesting    = rddTesting.count()\n",
    "\n",
    "print(\"Training: %d, validation: %d, test: %d\" % (rddTraining.count(), nbValidating, rddTesting.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#[START how_far]\n",
    "def howFarAreWe(model, against, sizeAgainst):\n",
    "  # Ignore the rating column  \n",
    "    againstNoRatings = against.map(lambda x: (int(x[0]), int(x[1])) )\n",
    "\n",
    "  # Keep the rating to compare against\n",
    "    againstWiRatings = against.map(lambda x: ((int(x[0]),int(x[1])), int(x[2])) )\n",
    "\n",
    "  # Make a prediction and map it for later comparison\n",
    "  # The map has to be ((user,product), rating) not ((product,user), rating)\n",
    "    predictions = model.predictAll(againstNoRatings).map(lambda p: ( (p[0],p[1]), p[2]) )\n",
    "\n",
    "  # Returns the pairs (prediction, rating)\n",
    "    predictionsAndRatings = predictions.join(againstWiRatings).values()    \n",
    "  # Returns the variance\n",
    "    return sqrt(predictionsAndRatings.map(lambda s: (s[0] - s[1]) ** 2).reduce(add) / float(sizeAgainst))\n",
    "#[END how_far]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best so far:0.956083\n",
      "Rank 5\n",
      "Regul 0.100000\n",
      "Iter 5\n",
      "Dist 0.956083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Rank 5\\nRegul 0.100000\\nIter 20\\nDist 0.959326 '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best results are not commented\n",
    "ranks  = [5,10,15,20]\n",
    "reguls = [0.1, 1,10]\n",
    "iters  = [5,10,20]\n",
    "\n",
    "finalModel = None\n",
    "finalRank  = 0\n",
    "finalRegul = float(0)\n",
    "finalIter  = -1\n",
    "finalDist   = float(100)\n",
    "\n",
    "#[START train_model]\n",
    "for cRank, cRegul, cIter in itertools.product(ranks, reguls, iters):\n",
    "    model = ALS.train(rddTraining, cRank, cIter, float(cRegul))\n",
    "    dist = howFarAreWe(model, rddValidating, nbValidating)\n",
    "    if dist < finalDist:\n",
    "        print(\"Best so far:%f\" % dist)\n",
    "        finalModel = model\n",
    "        finalRank  = cRank\n",
    "        finalRegul = cRegul\n",
    "        finalIter  = cIter\n",
    "        finalDist  = dist\n",
    "        break\n",
    "#[END train_model]\n",
    "\n",
    "print(\"Rank %i\" % finalRank) \n",
    "print(\"Regul %f\" % finalRegul) \n",
    "print(\"Iter %i\" % finalIter)  \n",
    "print(\"Dist %f\" % finalDist) \n",
    "\n",
    "\"\"\"Rank 5\n",
    "Regul 0.100000\n",
    "Iter 20\n",
    "Dist 0.959326 \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing data the RMSE is 0.990409526588\n"
     ]
    }
   ],
   "source": [
    "# Calculate all predictions\n",
    "rddTesting_withoutRating = rddTesting.map(lambda r: ((r[0], r[1])))\n",
    "predictions = model.predictAll(rddTesting_withoutRating).map(lambda r: ((r[0], r[1]), (r[2])))\n",
    "predictions.take(3)\n",
    "# user id, node_id, actual ratings,pred ratings -> df below\n",
    "rates_and_preds = rddTesting.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions) \n",
    "rates_and_preds.take(3)\n",
    "error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "    \n",
    "print 'For testing data the RMSE is %s' % (error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+------------------+\n",
      "| _1|   _2| _3|                _4|\n",
      "+---+-----+---+------------------+\n",
      "|652| 3466|4.0|3.0382546769711913|\n",
      "|563|44191|4.0|3.9195343187988336|\n",
      "|239| 2805|2.0|3.8174192794860202|\n",
      "|468|55830|1.0| 2.272269418311961|\n",
      "+---+-----+---+------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "x = rates_and_preds.map(lambda x : (x[0][0],x[0][1],x[1][0],x[1][1]))\n",
    "hasattr(x, \"toDF\")\n",
    "x.toDF().show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get total rating and average rating given to each movie by different users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_counts_and_averages(ID_and_ratings_tuple):    \n",
    "    nratings = len(ID_and_ratings_tuple[1]) \n",
    "    return ID_and_ratings_tuple[0], (nratings, sum([float(val) for val in ID_and_ratings_tuple[1]])/nratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 107), (97328, 1), (4, 13)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_ID_with_ratings_RDD = (ratings_data.map(lambda x: (x[1], x[2])).groupByKey())\n",
    "movie_ID_with_ratings_RDD_updated = movie_ID_with_ratings_RDD.map(lambda x : (x[0], list(x[1])))\n",
    "movie_ID_with_avg_ratings_RDD = movie_ID_with_ratings_RDD_updated.map(get_counts_and_averages)  # count and average rating\n",
    "movie_rating_counts_RDD = movie_ID_with_avg_ratings_RDD.map(lambda x: (int(x[0]), x[1][0]))    # rating count per movie\n",
    "movie_rating_counts_RDD.cache()\n",
    "movie_rating_counts_RDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model is  <pyspark.mllib.recommendation.MatrixFactorizationModel object at 0x7f95e16625d0>\n",
      "[(1084, (68, 3.2472970943697335)), (1084, (384, 3.267616431660904)), (1084, (440, 3.6721395703361432)), (1084, (4, 4.7272411413689905)), (1084, (600, 3.487342021216037)), (1084, (324, 3.456354607922227)), (1084, (180, 3.4447698460790717)), (1084, (340, 3.609319500741269)), (1084, (320, 3.3474256182772333)), (1084, (412, 2.812540921278155))]\n"
     ]
    }
   ],
   "source": [
    "# get user-wise movies watched\n",
    "all_users_ratings_RDD = ratings_data.map(lambda x: (x[0], x[1])).groupByKey()\n",
    "all_users_ratings_RDD = all_users_ratings_RDD.map(lambda x : (x[0], list(x[1])))    # movies watched by each user\n",
    "\n",
    "### finding unrated movies by each user- we will use this set for model's prediction/recommendations\n",
    "movie_ids = set(movies_data.map(lambda x : x[0]).toLocalIterator()) # list of all movie ids\n",
    "unrated_movies_RDD = all_users_df_ratings_RDD.map(lambda x: (x[0], list((movie_ids) - set(x[1]))))\n",
    "\n",
    "# #create user_id and unrated course id pairs\n",
    "unrated_usermovies_RDD = unrated_movies_RDD.flatMap(lambda x : [(x[0],i) for i in x[1]])\n",
    "\n",
    "# # #model predictions for each user and unrated course pairs\n",
    "print \"model is \", model\n",
    "recommendations_RDD = model.predictAll(unrated_usermovies_RDD)\n",
    "recommended_movies_rating_RDD = recommendations_RDD.map(lambda x: (x.product,(x.user, x.rating)))\n",
    "recommended_movies_rating_RDD.cache()\n",
    "print recommended_movies_rating_RDD.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining movie title and total number of ratings received by each movie for further filtering recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(68, (u'\"Great Mouse Detective', 2.9, 4)),\n",
       " (384, (u'\"Great Mouse Detective', 2.83, 4)),\n",
       " (440, (u'\"Great Mouse Detective', 3.13, 4))]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #     # converting id into int for course_ratings_count RDD to perform join\n",
    "# movie_rating_counts_RDD_updated = movie_rating_counts_RDD.map(lambda x: (int(x[0]), x[1]))\n",
    "\n",
    "# join movie name with movie id, predicted rating for movie and total number of ratings received by each movie\n",
    "recommendations_rating_title_and_count_RDD = recommended_movies_rating_RDD.join(movies_data).join(movie_rating_counts_RDD)\n",
    "recommendations_rating_title_and_count_RDD = recommendations_rating_title_and_count_RDD.map(lambda r: (r[1][0][1], r[1][0][0][0],round(r[1][0][0][1],2),r[1][1]))\n",
    "recommendations_rating_title_and_count_RDD = recommendations_rating_title_and_count_RDD.map(lambda x: (x[1],(x[0],x[2],x[3])))\n",
    "recommendations_rating_title_and_count_RDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Top 5 recommendations **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------------+-------------+--------------------+----------+\n",
      "|user_id|     recommendations|predicted_ratings|total_ratings|            rec_date|rec_number|\n",
      "+-------+--------------------+-----------------+-------------+--------------------+----------+\n",
      "|     26|Midnight in Paris...|             4.46|           21|2018-02-25 15:34:...|         1|\n",
      "|     26| Moulin Rouge (2001)|             4.09|           55|2018-02-25 15:34:...|         2|\n",
      "|     26|      \"African Queen|             4.08|           50|2018-02-25 15:34:...|         3|\n",
      "|     26|      Serpico (1973)|             4.06|           27|2018-02-25 15:34:...|         4|\n",
      "|     26|      Tangled (2010)|              4.0|           21|2018-02-25 15:34:...|         5|\n",
      "|     29|   \"Name of the Rose|             4.35|           23|2018-02-25 15:34:...|         1|\n",
      "|     29|Midnight in Paris...|             4.31|           21|2018-02-25 15:34:...|         2|\n",
      "|     29|\"Lord of the Ring...|             4.29|          176|2018-02-25 15:34:...|         3|\n",
      "|     29|The Imitation Gam...|             4.26|           32|2018-02-25 15:34:...|         4|\n",
      "|     29|     Serenity (2005)|             4.21|           40|2018-02-25 15:34:...|         5|\n",
      "+-------+--------------------+-----------------+-------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter only those courses which have been rated by atleast 20 users\n",
    "# take only top5 courses by sorting based on predicted ratings\n",
    "top_movies = recommendations_rating_title_and_count_RDD.groupBy(lambda x : x[0])\\\n",
    "                               .map(lambda x : list(x[1]))\\\n",
    "                               .map(lambda r: [i for i in r if i[1][2] > 20])\\\n",
    "                               .map(lambda a: [i for i in sorted(a, key=lambda x: -x[1][1])[:5]])   \n",
    "\n",
    "#preparing dataframe to insert in Database\n",
    "rec_movies_df = top_movies.map(lambda x: [(i[0],i[1][0],i[1][1],i[1][2]) for i in x]).flatMap(lambda x: x).toDF()\\\n",
    "                                .withColumnRenamed(\"_1\", \"user_id\")\\\n",
    "                                .withColumnRenamed(\"_2\", 'recommendations')\\\n",
    "                                .withColumnRenamed(\"_3\", 'predicted_ratings')\\\n",
    "                                .withColumnRenamed(\"_4\", \"total_ratings\")\n",
    "                \n",
    "#final recommendation engine dataframe to be saved in Database\n",
    "final_df_rec_eng = rec_movies_df.withColumn(\"rec_date\", sf.lit(datetime.datetime.now()).cast(TimestampType()))   \n",
    "final_df_rec_eng = final_df_rec_eng.withColumn(\"rec_number\", sf.row_number().over(Window.partitionBy(\"user_id\").orderBy(desc(\"predicted_ratings\"))))    \n",
    "\n",
    "final_df_rec_eng.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from pyspark.mllib.recommendation import MatrixFactorizationModel\n",
    "\n",
    "# model_path = os.path.join('..', 'models', 'movie_lens_als')\n",
    "\n",
    "# # Save and load model\n",
    "# model.save(sc, model_path)\n",
    "# same_model = MatrixFactorizationModel.load(sc, model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
